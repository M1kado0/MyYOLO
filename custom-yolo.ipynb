{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T19:18:57.156490Z",
     "iopub.status.busy": "2025-09-18T19:18:57.156234Z",
     "iopub.status.idle": "2025-09-18T19:19:00.450213Z",
     "shell.execute_reply": "2025-09-18T19:19:00.449374Z",
     "shell.execute_reply.started": "2025-09-18T19:18:57.156459Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics --quiet\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import timm, torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import batched_nms\n",
    "import numpy\n",
    "import math\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "import copy\n",
    "from time import time\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from dataset import *\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.03)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.cv1 = Conv(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.cv2 = Conv(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.add = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        x = self.cv1(x)\n",
    "        x = self.cv2(x)\n",
    "        if self.add:\n",
    "            x += x_in\n",
    "        return x\n",
    "        \n",
    "\n",
    "class C2f(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_bottlenecks, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.mid_channels = out_channels // 2\n",
    "        self.num_bottlenecks = num_bottlenecks\n",
    "        self.cv1 = Conv(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.cv2 = Conv((num_bottlenecks+2)*out_channels//2, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.m = nn.ModuleList([Bottleneck(self.mid_channels, self.mid_channels, shortcut) for _ in range(num_bottlenecks)]) # n bottlenecks\n",
    "        self.add = shortcut\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        x1, x2 = x[:, :x.shape[1]//2, :, :], x[:, x.shape[1]//2:, :, :]\n",
    "        outputs = [x1, x2] # x1 is fed to the bottlenecks\n",
    "\n",
    "        for i in range(self.num_bottlenecks):\n",
    "            x1 = self.m[i](x1)\n",
    "            outputs.insert(0, x1)\n",
    "        \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        out = self.cv2(outputs)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class SPPF(nn.Module): # EXPLORE WHY!!!!\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5): #kernel_size = size of maxpool\n",
    "        super().__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.cv1 = Conv(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0) # WHY???\n",
    "        self.pool = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size//2, dilation=1, ceil_mode=False) # WHY???\n",
    "        self.cv2 = Conv(4*hidden_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "\n",
    "        y1 = self.pool(x)\n",
    "        y2 = self.pool(y1)\n",
    "        y3 = self.pool(y2)\n",
    "\n",
    "        y = torch.cat([x,y1,y2,y3], dim=1)\n",
    "        \n",
    "        y = self.cv2(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    def __init__(self, dim=1): \n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, xs):       # xs is a tuple/list of tensors\n",
    "        return torch.cat(xs, self.dim)\n",
    "\n",
    "\n",
    "\n",
    "class DFL(nn.Module):\n",
    "    def __init__(self, ch=16):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.conv = nn.Conv2d(in_channels=ch, out_channels=1, kernel_size=1, stride=1, padding=0, bias=False).requires_grad_(False)\n",
    "\n",
    "        x = torch.arange(self.ch, dtype=torch.float).view(1, self.ch, 1, 1)\n",
    "        self.conv.weight.data.copy_(x)\n",
    "    \n",
    "    def forward(self, x): # x = [B, C_in, c]\n",
    "        b, c, a = x.shape # b = B  c = C_in = 4*ch  a = c\n",
    "        x = x.view(b, 4, self.ch, a).transpose(1, 2)  # [B, ch(values), 4, c]\n",
    "\n",
    "        x = x.softmax(1)  # [B, ch(softmax values), 4, c]\n",
    "        x = self.conv(x)  # [B, 1, 4, c]\n",
    "        return x.view(b, 4, a)  # [B, 4, c] so it returns the l,t,r,b values(in bin) for every batch (we don't need the out channel of conv)\n",
    "\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, ch=16, nc=4):\n",
    "        super().__init__()\n",
    "        self.ch=ch                          # dfl channels\n",
    "        self.box_ch=self.ch*4          # number of bounding boxes coordinates\n",
    "        self.nc=nc                 # 4 for our dataset\n",
    "        self.no=self.box_ch+self.nc    # num of outputs per anchor box\n",
    "        self.stride=torch.tensor([8.,16.,32])          # strides computed during build\n",
    "        d,w,r = (1/3,1/4,2.0)\n",
    "\n",
    "        self.cv2=nn.ModuleList([\n",
    "            # for box\n",
    "            nn.Sequential(Conv(int(256*w), self.box_ch, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.box_ch, self.box_ch, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.box_ch, self.box_ch, kernel_size=1, stride=1, padding=0)),\n",
    "            \n",
    "            nn.Sequential(Conv(int(512*w), self.box_ch, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.box_ch, self.box_ch, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.box_ch, self.box_ch, kernel_size=1, stride=1, padding=0)),\n",
    "            \n",
    "            nn.Sequential(Conv(int(512*w*r), self.box_ch, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.box_ch, self.box_ch, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.box_ch, self.box_ch, kernel_size=1, stride=1, padding=0)),\n",
    "        ])\n",
    "\n",
    "        # for classification\n",
    "        self.cv3=nn.ModuleList([\n",
    "            nn.Sequential(Conv(int(256*w), self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.nc, self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)),\n",
    "            \n",
    "            nn.Sequential(Conv(int(512*w), self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.nc, self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)),\n",
    "            \n",
    "            nn.Sequential(Conv(int(512*w*r), self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.nc, self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)),\n",
    "        ])\n",
    "\n",
    "        # dfl\n",
    "        self.dfl = DFL()\n",
    "\n",
    "    def forward(self, x): # x = (out1,out2,out3), outx = [B, chx, wx, hx]\n",
    "        outs = []\n",
    "        for i in range(len(self.cv2)):\n",
    "            box = self.cv2[i](x[i])     # [b, 4*reg_bins, w, h]\n",
    "            cls = self.cv3[i](x[i])     # [b, num_classes, w, h]\n",
    "            o = torch.cat((box, cls), 1)\n",
    "            outs.append(o) # [b, 4*reg_bins+num_classes, w, h] \n",
    "\n",
    "        # in training no dfl output\n",
    "        # if self.training:\n",
    "        return outs    # [3,b,4*reg_bins+num_classes,w,h]\n",
    "\n",
    "        \n",
    "        \n",
    "        # in inference, dfl produces refined bounding box coordinates\n",
    "        # anchors, strides = (i.transpose(0, 1) for i in self.make_anchors(x, self.stride))\n",
    "\n",
    "        # x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], dim=2)\n",
    "\n",
    "        # box, cls = x.split(split_size=(4*self.ch, self.nc), dim=1)\n",
    "\n",
    "        # a, b = self.dfl(box).chunk(2, 1)    # a=b=[b,2*self.ch,sum_i(h[i]w[i])]\n",
    "        # a = anchors.unsqueeze(0) - a\n",
    "        # b = anchors.unsqueeze(0) + b\n",
    "        # box = torch.cat(tensors=((a + b) / 2, b - a), dim=1)\n",
    "\n",
    "        # return torch.cat(tensors=(box * strides, cls.sigmoid()), dim=1)\n",
    "    \n",
    "    def make_anchors(self, x, strides, offset=0.5):\n",
    "        assert x is not None\n",
    "        anchor_tensor, stride_tensor = [], []\n",
    "        dtype, device = x[0].dtype, x[0].device\n",
    "        for i, stride in enumerate(strides):\n",
    "            _, _, h, w = x[i].shape\n",
    "            sx = torch.arange(end=w, device=device, dtype=dtype) + offset \n",
    "            sy = torch.arange(end=h, device=device, dtype=dtype) + offset\n",
    "            sy, sx = torch.meshgrid(sy, sx)\n",
    "            anchor_tensor.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "            stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
    "        return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n",
    "    \n",
    "\n",
    "\n",
    "class YOLO(nn.Module):\n",
    "    def __init__(self, task=None, verbose=False, in_channels=3):\n",
    "        super().__init__()\n",
    "        d, w, r = (1/3,1/4,2.0)\n",
    "        self.predictor = None  # reuse predictor\n",
    "        self.model = None  # model object\n",
    "        self.trainer = None  # trainer object\n",
    "        self.ckpt = {}  # if loaded from *.pt\n",
    "        self.cfg = None  # if loaded from *.yaml\n",
    "        self.ckpt_path = None\n",
    "        self.overrides = {}  # overrides for trainer object\n",
    "        self.metrics = None  # validation/training metrics\n",
    "        self.session = None  # HUB session\n",
    "        self.task = task  # task type\n",
    "        self.model_name = None  # model name\n",
    "        self.model = nn.ModuleList([\n",
    "            # backbone\n",
    "            Conv(in_channels, int(64*w), kernel_size=3, stride=2, padding=1),              #0\n",
    "            Conv(int(64*w), int(128*w), kernel_size=3, stride=2, padding=1),               #1\n",
    "            C2f(int(128*w), int(128*w), num_bottlenecks=int(3*d), shortcut=True),          #2\n",
    "            Conv(int(128*w), int(256*w), kernel_size=3, stride=2, padding=1),              #3\n",
    "            C2f(int(256*w), int(256*w), num_bottlenecks=int(6*d), shortcut=True),          #4\n",
    "            Conv(int(256*w), int(512*w), kernel_size=3, stride=2, padding=1),              #5\n",
    "            C2f(int(512*w), int(512*w), num_bottlenecks=int(6*d), shortcut=True),          #6\n",
    "            Conv(int(512*w), int(512*w*r), kernel_size=3, stride=2, padding=1),            #7\n",
    "            C2f(int(512*w*r), int(512*w*r), num_bottlenecks=int(3*d), shortcut=True),      #8\n",
    "            SPPF(int(512*w*r), int(512*w*r)),                                              #9\n",
    "\n",
    "            # neck\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),                                   #10\n",
    "            Concat(),                                                                      #11\n",
    "            C2f(int(512*w*(1+r)), int(512*w), num_bottlenecks=int(3*d), shortcut=False),   #12\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),                                   #13\n",
    "            Concat(),                                                                      #14\n",
    "            C2f(int(768*w), int(256*w), num_bottlenecks=int(3*d), shortcut=False),         #15\n",
    "            Conv(int(256*w), int(256*w), kernel_size=3, stride=2, padding=1),              #16\n",
    "            Concat(),                                                                      #17\n",
    "            C2f(int(768*w), int(512*w), num_bottlenecks=int(3*d), shortcut=False),         #18\n",
    "            Conv(int(512*w), int(512*w), kernel_size=3, stride=2, padding=1),              #19\n",
    "            Concat(),                                                                      #20\n",
    "            C2f(int(512*w*(1+r)), int(512*w*r), num_bottlenecks=int(3*d), shortcut=False), #21\n",
    "\n",
    "            # head\n",
    "            Detect(),                                                                      #22\n",
    "        ])\n",
    "        # Delete super().training for accessing self.model.training\n",
    "        del self.training\n",
    "\n",
    "    def forward(self, x):\n",
    "        # backbone forward\n",
    "        x = self.model[0](x)\n",
    "        x = self.model[1](x)\n",
    "        x = self.model[2](x)\n",
    "        x = self.model[3](x)\n",
    "        out1 = self.model[4](x) # for concat\n",
    "        x = self.model[5](out1)\n",
    "        out2 = self.model[6](x) # for concat\n",
    "        x = self.model[7](out2)\n",
    "        x = self.model[8](x)\n",
    "        out3 = self.model[9](x)\n",
    "\n",
    "        # neck forward\n",
    "        res_1 = out3 # for residual connection\n",
    "        x = self.model[10](out3)\n",
    "        x = self.model[11]((x, out2))\n",
    "        res_2 = self.model[12](x) # for concat\n",
    "        x = self.model[13](res_2)\n",
    "        x = self.model[14]((x, out1))\n",
    "        x1 = self.model[15](x) # for detect\n",
    "        x = self.model[16](x1)\n",
    "        x = self.model[17]((x, res_2))\n",
    "        x2 = self.model[18](x) # for detect\n",
    "        x = self.model[19](x2)\n",
    "        x = self.model[20]((x, res_1))\n",
    "        x3 = self.model[21](x) # for detect\n",
    "\n",
    "        return self.model[22]([x1,x2,x3])\n",
    "    \n",
    "    def _decode_for_nms(self, outputs):\n",
    "        detect_module = self.model[22]          # your Detect head\n",
    "        B = outputs[0].shape[0]\n",
    "        no = detect_module.no                   # box_ch + nc\n",
    "\n",
    "        # (B, no, N) by flattening each head map safely\n",
    "        x = torch.cat([o.reshape(B, -1, no).permute(0, 2, 1) for o in outputs], dim=2)\n",
    "\n",
    "        # split l,t,r,b distribution and class logits\n",
    "        boxes_dist, scores = x.split((detect_module.box_ch, detect_module.nc), dim=1)\n",
    "\n",
    "        # anchors:(N,2), strides:(N,) → make broadcastable\n",
    "        anchors, strides = detect_module.make_anchors(outputs, detect_module.stride)\n",
    "        anchors = anchors.transpose(0, 1).unsqueeze(0)   # (1,2,N)\n",
    "        strides = strides.view(1, 1, -1)                 # (1,1,N)\n",
    "\n",
    "        # DFL → (B,4,N) of l,t,r,b (grid/anchor units)\n",
    "        dist = detect_module.dfl(boxes_dist)\n",
    "        lt, rb = dist.split(2, dim=1)                    # (B,2,N), (B,2,N)\n",
    "\n",
    "        # XYXY in pixels of the network input (after resize/letterbox)\n",
    "        xyxy = torch.cat((anchors - lt, anchors + rb), dim=1) * strides  # (B,4,N)\n",
    "\n",
    "        # confidences (if your head has objectness, multiply it in here)\n",
    "        conf = scores.sigmoid()                          # (B,nc,N)\n",
    "\n",
    "        return torch.cat((xyxy, conf), dim=1)            # (B, 4+nc, N)\n",
    "\n",
    "    def train(self, mode: bool | None = None, **ultra_kwargs):\n",
    "        # 1) Pure PyTorch toggle if mode is given and no kwargs\n",
    "        if (mode is not None) and (not ultra_kwargs):\n",
    "            return super().train(mode)\n",
    "\n",
    "        # 2) Ultralytics-style API when kwargs are provided\n",
    "        if ultra_kwargs:\n",
    "            return self._ultra_train(**ultra_kwargs)\n",
    "\n",
    "        # 3) Default: behave like model.train(True)\n",
    "        return super().train(True)\n",
    "\n",
    "    # ---- the high-level trainer ----\n",
    "    def _ultra_train(\n",
    "        self,\n",
    "        data: str,\n",
    "        epochs: int = 100,\n",
    "        imgsz: int = 640,\n",
    "        batch: int = 16,\n",
    "        name: str = \"exp\",\n",
    "        project: str = \"runs/train\",\n",
    "        device: int | str = 0,\n",
    "        patience: int = 50,\n",
    "        cos_lr: bool = True,\n",
    "        lr: float = 5e-4,\n",
    "        weight_decay: float = 5e-4,\n",
    "        workers: int = 8,\n",
    "        amp: bool = True,\n",
    "        grad_clip: float | None = 10.0,\n",
    "    ):\n",
    "        if isinstance(device, (int, str)) and str(device).isdigit() and torch.cuda.is_available():\n",
    "            device = torch.device(f\"cuda:{device}\")\n",
    "        else:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        run_dir = os.path.join(project, name)\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        best_path, last_path = os.path.join(run_dir, \"best.pt\"), os.path.join(run_dir, \"last.pt\")\n",
    "\n",
    "        with open(data, \"r\") as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        nc = cfg.get(\"nc\", len(cfg[\"names\"]))\n",
    "        names = cfg.get(\"names\")\n",
    "\n",
    "        base = Path(data)\n",
    "        p = Path(cfg['train'])\n",
    "        newP = (base / p).resolve()\n",
    "\n",
    "        train_ds = YoloDataset((base / Path(cfg[\"train\"])).resolve(), imgsz=imgsz, names=names)           # <-- your class\n",
    "        val_path = cfg.get(\"val\")\n",
    "        val_ds   = YoloDataset((base / Path(val_path)).resolve(), imgsz=imgsz, names=names) if val_path else None\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True,\n",
    "                                  num_workers=workers, pin_memory=True,\n",
    "                                  collate_fn=train_ds.collate_fn)\n",
    "        val_loader = None\n",
    "        if val_ds:\n",
    "            val_loader = DataLoader(val_ds, batch_size=batch, shuffle=False,\n",
    "                                    num_workers=max(1, workers//2), pin_memory=True,\n",
    "                                    collate_fn=val_ds.collate_fn, drop_last=False)\n",
    "\n",
    "        def _gpu_mem_str(device):\n",
    "            if device.type == \"cuda\":\n",
    "                try:\n",
    "                    m = torch.cuda.memory_reserved(device.index if device.index is not None else 0)\n",
    "                except Exception:\n",
    "                    m = torch.cuda.max_memory_allocated()\n",
    "                return f\"{m / (1024**3):>7.2f}G\"\n",
    "            return f\"{0.0:>7.2f}G\"\n",
    "        \n",
    "        class _EMAval:\n",
    "            def __init__(self, beta=0.9): self.b=beta; self.v=None\n",
    "            def upd(self,x): x=float(x); self.v=x if self.v is None else self.b*self.v+(1-self.b)*x; return self.v\n",
    "        \n",
    "        def _epoch_header():\n",
    "            print(f\"{'Epoch':>10} {'GPU_mem':>9} {'box_loss':>9} {'cls_loss':>9} {'dfl_loss':>9} {'Instances':>10} {'Size':>10}\")\n",
    "\n",
    "        # loss, opt, sched\n",
    "\n",
    "        params = {\n",
    "            'min_lr': 0.000100000000,\n",
    "            'max_lr': 0.010000000000,\n",
    "            'momentum': 0.9370000000,\n",
    "            'weight_decay': 0.000500,\n",
    "            'warmup_epochs': 3.00000,\n",
    "            'box': 7.500000000000000,\n",
    "            'cls': 0.500000000000000,\n",
    "            'dfl': 1.500000000000000,\n",
    "            'hsv_h': 0.0150000000000,\n",
    "            'hsv_s': 0.7000000000000,\n",
    "            'hsv_v': 0.4000000000000,\n",
    "            'degrees': 0.00000000000,\n",
    "            'translate': 0.100000000,\n",
    "            'scale': 0.5000000000000,\n",
    "            'shear': 0.0000000000000,\n",
    "            'flip_ud': 0.00000000000,\n",
    "            'flip_lr': 0.50000000000,\n",
    "            'mosaic': 1.000000000000,\n",
    "            'mix_up': 0.000000000000\n",
    "        }\n",
    "        params['nc'] = nc\n",
    "        params['names'] = names\n",
    "        \n",
    "        criterion = ComputeLoss(self.model, params)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = (torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "                     if cos_lr else torch.optim.lr_scheduler.MultiStepLR(optimizer, [int(0.8*epochs)], gamma=0.1))\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(amp and device.type == \"cuda\"))\n",
    "\n",
    "        # loop with early stopping on val loss\n",
    "        best_val = float(\"inf\"); bad_epochs = 0\n",
    "        for epoch in range(epochs):\n",
    "            super().train(True)  # PyTorch training mode\n",
    "            _epoch_header()\n",
    "            eb, ec, ed = _EMAval(), _EMAval(), _EMAval()\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            pbar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True, ncols=120,\n",
    "                bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\")\n",
    "            \n",
    "            for i, (imgs, targets) in pbar:\n",
    "                imgs = imgs.to(device, non_blocking=True).float() / 255.0\n",
    "                instances = int(targets[\"cls\"].numel())\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.autocast(\"cuda\", dtype=torch.float16, enabled=(amp and device.type == \"cuda\")):\n",
    "                    outputs = self(imgs)  # training path → 3 feature maps\n",
    "                    box_loss, cls_loss, dfl_loss = criterion(outputs, targets)\n",
    "                    loss = box_loss + cls_loss + dfl_loss\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                if grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                b, c, d = eb.upd(box_loss.item()), ec.upd(cls_loss.item()), ed.upd(dfl_loss.item())\n",
    "                desc = f\"{epoch+1:>7}/{epochs:<3} {_gpu_mem_str(device)} {b:>9.3f} {c:>9.3f} {d:>9.3f} {instances:>10d} {imgsz:>10d}:\"\n",
    "                pbar.set_description_str(desc)\n",
    "    \n",
    "\n",
    "            scheduler.step()\n",
    "            train_loss = epoch_loss / max(1, len(train_loader))\n",
    "\n",
    "            val_loss = train_loss\n",
    "            if val_loader is not None:\n",
    "                super().eval()\n",
    "                val_total = 0.0\n",
    "                # header above the val bar\n",
    "                print((\" \" * 17) + \"Class     Images  Instances      Box(P          R      mAP50  mAP50-95):\", end=\" \")\n",
    "                \n",
    "                vbar = tqdm(enumerate(val_loader), total=len(val_loader), leave=True, ncols=120,\n",
    "                    bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\")\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for imgs, targets in val_loader:\n",
    "                        imgs = imgs.to(device, non_blocking=True).float() / 255.0\n",
    "                        outputs = self(imgs)  # eval forward (no grads)\n",
    "\n",
    "                        vb, vc, vd = criterion(outputs, targets)  # same criterion & weights as train\n",
    "                        val_total += (vb + vc + vd).item()\n",
    "\n",
    "                val_loss = val_total / max(1, len(val_loader))\n",
    "\n",
    "            if val_loader is not None:\n",
    "                print((\" \" * 17) + \"Class     Images  Instances      Box(P          R      mAP50  mAP50-95):\", end=\" \")\n",
    "\n",
    "                iouv = torch.linspace(0.5, 0.95, 10, device=device)\n",
    "                tp_list, conf_list, pcls_list, tcls_list = [], [], [], []\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for imgs, targets in tqdm(enumerate(val_loader),\n",
    "                                            total=len(val_loader), leave=True, ncols=120,\n",
    "                                            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\"):\n",
    "                        _, (imgs, targets) = _\n",
    "                        imgs = imgs.to(device, non_blocking=True).float() / 255.0\n",
    "\n",
    "                        # eval forward\n",
    "                        outputs = self(imgs)\n",
    "\n",
    "                        # decode for NMS (XYXY already)\n",
    "                        detect_module = self.model[22]\n",
    "                        processed_for_nms = _decode_for_nms(outputs, detect_module)\n",
    "\n",
    "                        # run NMS (expects shape (B, 4+nc, N))\n",
    "                        preds = non_max_suppression(processed_for_nms, confidence_threshold=0.001, iou_threshold=0.7)\n",
    "\n",
    "                        B, _, H, W = imgs.shape\n",
    "                        for b in range(B):\n",
    "                            p = preds[b]  # [n_det, 6] = [x1,y1,x2,y2,conf,cls]\n",
    "                            # ---- build GT in the SAME coordinate frame as preds ----\n",
    "                            m = (targets[\"idx\"] == b)\n",
    "                            if m.any():\n",
    "                                gt_cls = targets[\"cls\"][m].to(device).squeeze(-1).to(torch.long)\n",
    "                                # If your dataset normalizes to the network input size, this is OK:\n",
    "                                gt_box = xywhn2xyxy_torch(targets[\"box\"][m].to(device).float(), W, H)\n",
    "                                # If you letterbox, ensure GT is mapped to the input frame (apply gain/pad).\n",
    "                                t = torch.zeros((gt_cls.shape[0], 5), device=device, dtype=torch.float32)\n",
    "                                t[:, 0] = gt_cls.float()\n",
    "                                t[:, 1:] = gt_box\n",
    "                            else:\n",
    "                                t = torch.zeros((0, 5), device=device)\n",
    "\n",
    "                            if p.numel() == 0:\n",
    "                                if t.numel():\n",
    "                                    tcls_list.append(t[:, 0].cpu().numpy().astype(int))\n",
    "                                continue\n",
    "\n",
    "                            # make sure classes are int for equality checks\n",
    "                            p[:, 5] = p[:, 5].to(torch.long).float()\n",
    "\n",
    "                            correct = compute_metric(p, t, iouv)  # [n_det, 10] bool\n",
    "                            tp_list.append(correct.cpu().numpy().astype(int))\n",
    "                            conf_list.append(p[:, 4].cpu().numpy())\n",
    "                            pcls_list.append(p[:, 5].cpu().numpy())\n",
    "                            if t.numel():\n",
    "                                tcls_list.append(t[:, 0].cpu().numpy().astype(int))\n",
    "\n",
    "                if conf_list:\n",
    "                    tp   = numpy.concatenate(tp_list, 0)\n",
    "                    conf = numpy.concatenate(conf_list, 0)\n",
    "                    pcls = numpy.concatenate(pcls_list, 0).astype(int)\n",
    "                    tcls = numpy.concatenate(tcls_list, 0).astype(int) if tcls_list else numpy.zeros((0,), dtype=int)\n",
    "                    _, _, P, R, mAP50, mAP5095 = compute_ap(tp, conf, pcls, tcls)\n",
    "                else:\n",
    "                    P = R = mAP50 = mAP5095 = float(\"nan\")\n",
    "                    tcls = numpy.zeros((0,), dtype=int)\n",
    "\n",
    "                val_images = len(val_loader.dataset)\n",
    "                val_instances = int(tcls.shape[0])\n",
    "                print(f\"\\n{'':>19}{'all':>7}{val_images:>11}{val_instances:>12}\"\n",
    "                    f\"{P:>11.3f}{R:>11.3f}{mAP50:>11.3f}{mAP5095:>11.3f}\\n\")\n",
    "        \n",
    "            # ------------------- Save & early stop -------------------\n",
    "            torch.save(self.state_dict(), last_path)\n",
    "            improved = val_loss < best_val - 1e-6\n",
    "            if improved:\n",
    "                best_val, bad_epochs = val_loss, 0\n",
    "                torch.save(self.state_dict(), best_path)\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "        \n",
    "            if bad_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        return self\n",
    "        # return {\"best_val_loss\": best_val, \"best\": best_path, \"last\": last_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !cp -r /kaggle/input/effyolo /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from ultralytics import YOLO as UModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "u = UModel('yolov8n.pt')          # COCO-pretrained\n",
    "model = YOLO().to(device)\n",
    "\n",
    "tgt_sd = model.state_dict()\n",
    "sd_src = u.model.state_dict()     # plain PyTorch state_dict\n",
    "sd_shape_match = OrderedDict((k, v) for k, v in sd_src.items()\n",
    "                             if k in tgt_sd and tgt_sd[k].shape == v.shape)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(sd_shape_match, strict=False)\n",
    "print(len(missing), len(unexpected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = model.train(\n",
    "    data='/kaggle/working/effyolo/data.yaml',\n",
    "    epochs=200,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    name='fruit-disease-detector',\n",
    "    project='/kaggle/working/runs/train',\n",
    "    device='cuda',\n",
    "    patience=50,\n",
    "    cos_lr=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/my_yolo8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8254015,
     "sourceId": 13035527,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
