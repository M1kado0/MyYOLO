{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:02:35.045512Z",
     "iopub.status.busy": "2025-09-16T14:02:35.045154Z",
     "iopub.status.idle": "2025-09-16T14:02:38.562356Z",
     "shell.execute_reply": "2025-09-16T14:02:38.561171Z",
     "shell.execute_reply.started": "2025-09-16T14:02:35.045490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics --quiet\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import timm, torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.ops import batched_nms\n",
    "import numpy\n",
    "import math\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "import copy\n",
    "from time import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:06:09.773455Z",
     "iopub.status.busy": "2025-09-16T14:06:09.773100Z",
     "iopub.status.idle": "2025-09-16T14:06:09.823944Z",
     "shell.execute_reply": "2025-09-16T14:06:09.823107Z",
     "shell.execute_reply.started": "2025-09-16T14:06:09.773432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp'\n",
    "\n",
    "\n",
    "class YoloDataset(data.Dataset):\n",
    "    def __init__(self, foldername=Path('../.'), imgsz=640, names={}, augment=False):\n",
    "        self.names = names\n",
    "        self.mosaic = augment\n",
    "        self.augment = augment\n",
    "        self.imgsz = imgsz\n",
    "\n",
    "        # Read labels\n",
    "        filenames = sorted(str(p) for p in foldername.rglob(\"*\") if p.is_file())\n",
    "        \n",
    "        labels = self.load_label(filenames)\n",
    "        self.labels = list(labels.values())\n",
    "        self.filenames = list(labels.keys())  # update\n",
    "        self.n = len(self.filenames)  # number of samples\n",
    "        self.indices = range(self.n)\n",
    "        # Albumentations (optional, only used if package is installed)\n",
    "        self.albumentations = Albumentations()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        index = self.indices[index]\n",
    "\n",
    "        if self.mosaic and random.random() < self.params['mosaic']:\n",
    "            # Load MOSAIC\n",
    "            image, label = self.load_mosaic(index, self.params)\n",
    "            # MixUp augmentation\n",
    "            if random.random() < self.params['mix_up']:\n",
    "                index = random.choice(self.indices)\n",
    "                mix_image1, mix_label1 = image, label\n",
    "                mix_image2, mix_label2 = self.load_mosaic(index, self.params)\n",
    "\n",
    "                image, label = mix_up(mix_image1, mix_label1, mix_image2, mix_label2)\n",
    "        else:\n",
    "            # Load image\n",
    "            image, shape = self.load_image(index)\n",
    "            h, w = image.shape[:2]\n",
    "\n",
    "            # Resize\n",
    "            image, ratio, pad = resize(image, self.input_size, self.augment)\n",
    "            label = self.labels[index].copy()\n",
    "            if label.size:\n",
    "                label[:, 1:] = xywhn2xyxy(label[:, 1:], ratio[0] * w, ratio[1] * h, pad[0], pad[1])\n",
    "            if self.augment:\n",
    "                image, label = random_perspective(image, label, self.params)\n",
    "\n",
    "        nl = len(label)  # number of labels\n",
    "        h, w = image.shape[:2]\n",
    "        cls = label[:, 0:1]\n",
    "        box = label[:, 1:5]\n",
    "        box = xyxy2xywhn(box, w, h)\n",
    "\n",
    "        if self.augment:\n",
    "            # Albumentations\n",
    "            image, box, cls = self.albumentations(image, box, cls)\n",
    "            nl = len(box)  # update after albumentations\n",
    "            # HSV color-space\n",
    "            augment_hsv(image, self.params)\n",
    "            # Flip up-down\n",
    "            if random.random() < self.params['flip_ud']:\n",
    "                image = numpy.flipud(image)\n",
    "                if nl:\n",
    "                    box[:, 1] = 1 - box[:, 1]\n",
    "            # Flip left-right\n",
    "            if random.random() < self.params['flip_lr']:\n",
    "                image = numpy.fliplr(image)\n",
    "                if nl:\n",
    "                    box[:, 0] = 1 - box[:, 0]\n",
    "\n",
    "        target_cls = torch.zeros((nl, 1))\n",
    "        target_box = torch.zeros((nl, 4))\n",
    "        if nl:\n",
    "            target_cls = torch.from_numpy(cls)\n",
    "            target_box = torch.from_numpy(box)\n",
    "\n",
    "        # Convert HWC to CHW, BGR to RGB\n",
    "        sample = image.transpose((2, 0, 1))[::-1]\n",
    "        sample = numpy.ascontiguousarray(sample)\n",
    "\n",
    "        return torch.from_numpy(sample), target_cls, target_box, torch.zeros(nl)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def load_image(self, i):\n",
    "        image = cv2.imread(self.filenames[i])\n",
    "        h, w = image.shape[:2]\n",
    "        r = self.input_size / max(h, w)\n",
    "        if r != 1:\n",
    "            image = cv2.resize(image,\n",
    "                               dsize=(int(w * r), int(h * r)),\n",
    "                               interpolation=resample() if self.augment else cv2.INTER_LINEAR)\n",
    "        return image, (h, w)\n",
    "\n",
    "    def load_mosaic(self, index, params):\n",
    "        label4 = []\n",
    "        border = [-self.input_size // 2, -self.input_size // 2]\n",
    "        image4 = numpy.full((self.input_size * 2, self.input_size * 2, 3), 0, dtype=numpy.uint8)\n",
    "        y1a, y2a, x1a, x2a, y1b, y2b, x1b, x2b = (None, None, None, None, None, None, None, None)\n",
    "\n",
    "        xc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "        yc = int(random.uniform(-border[0], 2 * self.input_size + border[1]))\n",
    "\n",
    "        indices = [index] + random.choices(self.indices, k=3)\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        for i, index in enumerate(indices):\n",
    "            # Load image\n",
    "            image, _ = self.load_image(index)\n",
    "            shape = image.shape\n",
    "            if i == 0:  # top left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = xc\n",
    "                y2a = yc\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = shape[1]\n",
    "                y2b = shape[0]\n",
    "            if i == 1:  # top right\n",
    "                x1a = xc\n",
    "                y1a = max(yc - shape[0], 0)\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = yc\n",
    "                x1b = 0\n",
    "                y1b = shape[0] - (y2a - y1a)\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = shape[0]\n",
    "            if i == 2:  # bottom left\n",
    "                x1a = max(xc - shape[1], 0)\n",
    "                y1a = yc\n",
    "                x2a = xc\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = shape[1] - (x2a - x1a)\n",
    "                y1b = 0\n",
    "                x2b = shape[1]\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "            if i == 3:  # bottom right\n",
    "                x1a = xc\n",
    "                y1a = yc\n",
    "                x2a = min(xc + shape[1], self.input_size * 2)\n",
    "                y2a = min(self.input_size * 2, yc + shape[0])\n",
    "                x1b = 0\n",
    "                y1b = 0\n",
    "                x2b = min(shape[1], x2a - x1a)\n",
    "                y2b = min(y2a - y1a, shape[0])\n",
    "\n",
    "            pad_w = x1a - x1b\n",
    "            pad_h = y1a - y1b\n",
    "            image4[y1a:y2a, x1a:x2a] = image[y1b:y2b, x1b:x2b]\n",
    "\n",
    "            # Labels\n",
    "            label = self.labels[index].copy()\n",
    "            if len(label):\n",
    "                label[:, 1:] = xywhn2xyxy(label[:, 1:], shape[1], shape[0], pad_w, pad_h)\n",
    "            label4.append(label)\n",
    "\n",
    "        # Concat/clip labels\n",
    "        label4 = numpy.concatenate(label4, 0)\n",
    "        for x in label4[:, 1:]:\n",
    "            numpy.clip(x, 0, 2 * self.input_size, out=x)\n",
    "\n",
    "        # Augment\n",
    "        image4, label4 = random_perspective(image4, label4, params, border)\n",
    "\n",
    "        return image4, label4\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        samples, cls, box, indices = zip(*batch)\n",
    "\n",
    "        cls = torch.cat(cls, dim=0)\n",
    "        box = torch.cat(box, dim=0)\n",
    "\n",
    "        new_indices = list(indices)\n",
    "        for i in range(len(indices)):\n",
    "            new_indices[i] += i\n",
    "        indices = torch.cat(new_indices, dim=0)\n",
    "\n",
    "        targets = {'cls': cls,\n",
    "                   'box': box,\n",
    "                   'idx': indices}\n",
    "        return torch.stack(samples, dim=0), targets\n",
    "\n",
    "    @staticmethod\n",
    "    def load_label(filenames):\n",
    "        path = f'{os.path.dirname(filenames[0])}.cache'\n",
    "        if os.path.exists(path):\n",
    "            return torch.load(path, weights_only=False)\n",
    "        x = {}\n",
    "        for filename in filenames:\n",
    "            try:\n",
    "                # verify images\n",
    "                with open(filename, 'rb') as f:\n",
    "                    image = Image.open(f)\n",
    "                    image.verify()  # PIL verify\n",
    "                shape = image.size  # image size\n",
    "                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
    "                assert image.format.lower() in FORMATS, f'invalid image format {image.format}'\n",
    "\n",
    "                # verify labels\n",
    "                a = f'{os.sep}images{os.sep}'\n",
    "                b = f'{os.sep}labels{os.sep}'\n",
    "                if os.path.isfile(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt'):\n",
    "                    with open(b.join(filename.rsplit(a, 1)).rsplit('.', 1)[0] + '.txt') as f:\n",
    "                        label = [x.split() for x in f.read().strip().splitlines() if len(x)]\n",
    "                        label = numpy.array(label, dtype=numpy.float32)\n",
    "                    nl = len(label)\n",
    "                    if nl:\n",
    "                        assert (label >= 0).all()\n",
    "                        assert label.shape[1] == 5\n",
    "                        assert (label[:, 1:] <= 1).all()\n",
    "                        _, i = numpy.unique(label, axis=0, return_index=True)\n",
    "                        if len(i) < nl:  # duplicate row check\n",
    "                            label = label[i]  # remove duplicates\n",
    "                    else:\n",
    "                        label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
    "                else:\n",
    "                    label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
    "            except FileNotFoundError:\n",
    "                label = numpy.zeros((0, 5), dtype=numpy.float32)\n",
    "            except AssertionError:\n",
    "                continue\n",
    "            x[filename] = label\n",
    "        torch.save(x, path)\n",
    "        return x\n",
    "\n",
    "\n",
    "def xywhn2xyxy(x, w=640, h=640, pad_w=0, pad_h=0):\n",
    "    # Convert nx4 boxes\n",
    "    # from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = numpy.copy(x)\n",
    "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + pad_w  # top left x\n",
    "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + pad_h  # top left y\n",
    "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + pad_w  # bottom right x\n",
    "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + pad_h  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def xyxy2xywhn(x, w, h):\n",
    "    # warning: inplace clip\n",
    "    x[:, [0, 2]] = x[:, [0, 2]].clip(0, w - 1E-3)  # x1, x2\n",
    "    x[:, [1, 3]] = x[:, [1, 3]].clip(0, h - 1E-3)  # y1, y2\n",
    "\n",
    "    # Convert nx4 boxes\n",
    "    # from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    y = numpy.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def resample():\n",
    "    choices = (cv2.INTER_AREA,\n",
    "               cv2.INTER_CUBIC,\n",
    "               cv2.INTER_LINEAR,\n",
    "               cv2.INTER_NEAREST,\n",
    "               cv2.INTER_LANCZOS4)\n",
    "    return random.choice(choices)\n",
    "\n",
    "\n",
    "def augment_hsv(image, params):\n",
    "    # HSV color-space augmentation\n",
    "    h = params['hsv_h']\n",
    "    s = params['hsv_s']\n",
    "    v = params['hsv_v']\n",
    "\n",
    "    r = numpy.random.uniform(-1, 1, 3) * [h, s, v] + 1\n",
    "    h, s, v = cv2.split(cv2.cvtColor(image, cv2.COLOR_BGR2HSV))\n",
    "\n",
    "    x = numpy.arange(0, 256, dtype=r.dtype)\n",
    "    lut_h = ((x * r[0]) % 180).astype('uint8')\n",
    "    lut_s = numpy.clip(x * r[1], 0, 255).astype('uint8')\n",
    "    lut_v = numpy.clip(x * r[2], 0, 255).astype('uint8')\n",
    "\n",
    "    hsv = cv2.merge((cv2.LUT(h, lut_h), cv2.LUT(s, lut_s), cv2.LUT(v, lut_v)))\n",
    "    cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR, dst=image)  # no return needed\n",
    "\n",
    "\n",
    "def resize(image, input_size, augment):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = image.shape[:2]  # current shape [height, width]\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(input_size / shape[0], input_size / shape[1])\n",
    "    if not augment:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    pad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    w = (input_size - pad[0]) / 2\n",
    "    h = (input_size - pad[1]) / 2\n",
    "\n",
    "    if shape[::-1] != pad:  # resize\n",
    "        image = cv2.resize(image,\n",
    "                           dsize=pad,\n",
    "                           interpolation=resample() if augment else cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(h - 0.1)), int(round(h + 0.1))\n",
    "    left, right = int(round(w - 0.1)), int(round(w + 0.1))\n",
    "    image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)  # add border\n",
    "    return image, (r, r), (w, h)\n",
    "\n",
    "\n",
    "def candidates(box1, box2):\n",
    "    # box1(4,n), box2(4,n)\n",
    "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
    "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
    "    aspect_ratio = numpy.maximum(w2 / (h2 + 1e-16), h2 / (w2 + 1e-16))  # aspect ratio\n",
    "    return (w2 > 2) & (h2 > 2) & (w2 * h2 / (w1 * h1 + 1e-16) > 0.1) & (aspect_ratio < 100)\n",
    "\n",
    "\n",
    "def random_perspective(image, label, params, border=(0, 0)):\n",
    "    h = image.shape[0] + border[0] * 2\n",
    "    w = image.shape[1] + border[1] * 2\n",
    "\n",
    "    # Center\n",
    "    center = numpy.eye(3)\n",
    "    center[0, 2] = -image.shape[1] / 2  # x translation (pixels)\n",
    "    center[1, 2] = -image.shape[0] / 2  # y translation (pixels)\n",
    "\n",
    "    # Perspective\n",
    "    perspective = numpy.eye(3)\n",
    "\n",
    "    # Rotation and Scale\n",
    "    rotate = numpy.eye(3)\n",
    "    a = random.uniform(-params['degrees'], params['degrees'])\n",
    "    s = random.uniform(1 - params['scale'], 1 + params['scale'])\n",
    "    rotate[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
    "\n",
    "    # Shear\n",
    "    shear = numpy.eye(3)\n",
    "    shear[0, 1] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "    shear[1, 0] = math.tan(random.uniform(-params['shear'], params['shear']) * math.pi / 180)\n",
    "\n",
    "    # Translation\n",
    "    translate = numpy.eye(3)\n",
    "    translate[0, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * w\n",
    "    translate[1, 2] = random.uniform(0.5 - params['translate'], 0.5 + params['translate']) * h\n",
    "\n",
    "    # Combined rotation matrix, order of operations (right to left) is IMPORTANT\n",
    "    matrix = translate @ shear @ rotate @ perspective @ center\n",
    "    if (border[0] != 0) or (border[1] != 0) or (matrix != numpy.eye(3)).any():  # image changed\n",
    "        image = cv2.warpAffine(image, matrix[:2], dsize=(w, h), borderValue=(0, 0, 0))\n",
    "\n",
    "    # Transform label coordinates\n",
    "    n = len(label)\n",
    "    if n:\n",
    "        xy = numpy.ones((n * 4, 3))\n",
    "        xy[:, :2] = label[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
    "        xy = xy @ matrix.T  # transform\n",
    "        xy = xy[:, :2].reshape(n, 8)  # perspective rescale or affine\n",
    "\n",
    "        # create new boxes\n",
    "        x = xy[:, [0, 2, 4, 6]]\n",
    "        y = xy[:, [1, 3, 5, 7]]\n",
    "        box = numpy.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
    "\n",
    "        # clip\n",
    "        box[:, [0, 2]] = box[:, [0, 2]].clip(0, w)\n",
    "        box[:, [1, 3]] = box[:, [1, 3]].clip(0, h)\n",
    "        # filter candidates\n",
    "        indices = candidates(box1=label[:, 1:5].T * s, box2=box.T)\n",
    "\n",
    "        label = label[indices]\n",
    "        label[:, 1:5] = box[indices]\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def mix_up(image1, label1, image2, label2):\n",
    "    # Applies MixUp augmentation https://arxiv.org/pdf/1710.09412.pdf\n",
    "    alpha = numpy.random.beta(a=32.0, b=32.0)  # mix-up ratio, alpha=beta=32.0\n",
    "    image = (image1 * alpha + image2 * (1 - alpha)).astype(numpy.uint8)\n",
    "    label = numpy.concatenate((label1, label2), 0)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "class Albumentations:\n",
    "    def __init__(self):\n",
    "        self.transform = None\n",
    "        try:\n",
    "            import albumentations\n",
    "\n",
    "            transforms = [albumentations.Blur(p=0.01),\n",
    "                          albumentations.CLAHE(p=0.01),\n",
    "                          albumentations.ToGray(p=0.01),\n",
    "                          albumentations.MedianBlur(p=0.01)]\n",
    "            self.transform = albumentations.Compose(transforms,\n",
    "                                                    albumentations.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "        except ImportError:  # package not installed, skip\n",
    "            pass\n",
    "\n",
    "    def __call__(self, image, box, cls):\n",
    "        if self.transform:\n",
    "            x = self.transform(image=image,\n",
    "                               bboxes=box,\n",
    "                               class_labels=cls)\n",
    "            image = x['image']\n",
    "            box = numpy.array(x['bboxes'])\n",
    "            cls = numpy.array(x['class_labels'])\n",
    "        return image, box, cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:06:21.677727Z",
     "iopub.status.busy": "2025-09-16T14:06:21.676952Z",
     "iopub.status.idle": "2025-09-16T14:06:22.032962Z",
     "shell.execute_reply": "2025-09-16T14:06:22.032060Z",
     "shell.execute_reply.started": "2025-09-16T14:06:21.677700Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_seed():\n",
    "    \"\"\"\n",
    "    Setup random seed.\n",
    "    \"\"\"\n",
    "    random.seed(0)\n",
    "    numpy.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def setup_multi_processes():\n",
    "    \"\"\"\n",
    "    Setup multi-processing environment variables.\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    from os import environ\n",
    "    from platform import system\n",
    "\n",
    "    # set multiprocess start method as `fork` to speed up the training\n",
    "    if system() != 'Windows':\n",
    "        torch.multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "    # disable opencv multithreading to avoid system being overloaded\n",
    "    cv2.setNumThreads(0)\n",
    "\n",
    "    # setup OMP threads\n",
    "    if 'OMP_NUM_THREADS' not in environ:\n",
    "        environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "    # setup MKL threads\n",
    "    if 'MKL_NUM_THREADS' not in environ:\n",
    "        environ['MKL_NUM_THREADS'] = '1'\n",
    "\n",
    "\n",
    "def export_onnx(args):\n",
    "    import onnx  # noqa\n",
    "\n",
    "    inputs = ['images']\n",
    "    outputs = ['outputs']\n",
    "    dynamic = {'outputs': {0: 'batch', 1: 'anchors'}}\n",
    "\n",
    "    m = torch.load('./weights/best.pt')['model'].float()\n",
    "    x = torch.zeros((1, 3, args.input_size, args.input_size))\n",
    "\n",
    "    torch.onnx.export(m.cpu(), x.cpu(),\n",
    "                      f='./weights/best.onnx',\n",
    "                      verbose=False,\n",
    "                      opset_version=12,\n",
    "                      # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False\n",
    "                      do_constant_folding=True,\n",
    "                      input_names=inputs,\n",
    "                      output_names=outputs,\n",
    "                      dynamic_axes=dynamic or None)\n",
    "\n",
    "    # Checks\n",
    "    model_onnx = onnx.load('./weights/best.onnx')  # load onnx model\n",
    "    onnx.checker.check_model(model_onnx)  # check onnx model\n",
    "\n",
    "    onnx.save(model_onnx, './weights/best.onnx')\n",
    "    # Inference example\n",
    "    # https://github.com/ultralytics/ultralytics/blob/main/ultralytics/nn/autobackend.py\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else numpy.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def make_anchors(x, strides, offset=0.5):\n",
    "    assert x is not None\n",
    "    anchor_tensor, stride_tensor = [], []\n",
    "    dtype, device = x[0].dtype, x[0].device\n",
    "    for i, stride in enumerate(strides):\n",
    "        _, _, h, w = x[i].shape\n",
    "        sx = torch.arange(end=w, device=device, dtype=dtype) + offset  # shift x\n",
    "        sy = torch.arange(end=h, device=device, dtype=dtype) + offset  # shift y\n",
    "        sy, sx = torch.meshgrid(sy, sx)\n",
    "        anchor_tensor.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "        stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
    "    return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n",
    "\n",
    "\n",
    "def compute_metric(output, target, iou_v):\n",
    "    # intersection(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    (a1, a2) = target[:, 1:].unsqueeze(1).chunk(2, 2)\n",
    "    (b1, b2) = output[:, :4].unsqueeze(0).chunk(2, 2)\n",
    "    intersection = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "    # IoU = intersection / (area1 + area2 - intersection)\n",
    "    iou = intersection / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - intersection + 1e-7)\n",
    "\n",
    "    correct = numpy.zeros((output.shape[0], iou_v.shape[0]))\n",
    "    correct = correct.astype(bool)\n",
    "    for i in range(len(iou_v)):\n",
    "        # IoU > threshold and classes match\n",
    "        x = torch.where((iou >= iou_v[i]) & (target[:, 0:1] == output[:, 5]))\n",
    "        if x[0].shape[0]:\n",
    "            matches = torch.cat((torch.stack(x, 1),\n",
    "                                 iou[x[0], x[1]][:, None]), 1).cpu().numpy()  # [label, detect, iou]\n",
    "            if x[0].shape[0] > 1:\n",
    "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "                matches = matches[numpy.unique(matches[:, 1], return_index=True)[1]]\n",
    "                matches = matches[numpy.unique(matches[:, 0], return_index=True)[1]]\n",
    "            correct[matches[:, 1].astype(int), i] = True\n",
    "    return torch.tensor(correct, dtype=torch.bool, device=output.device)\n",
    "\n",
    "\n",
    "def non_max_suppression(outputs, confidence_threshold=0.001, iou_threshold=0.7):\n",
    "    max_wh = 7680\n",
    "    max_det = 300\n",
    "    max_nms = 30000\n",
    "\n",
    "    bs = outputs.shape[0]  # batch size\n",
    "    nc = outputs.shape[1] - 4  # number of classes\n",
    "    xc = outputs[:, 4:4 + nc].amax(1) > confidence_threshold  # candidates\n",
    "\n",
    "    # Settings\n",
    "    start = time()\n",
    "    limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "    output = [torch.zeros((0, 6), device=outputs.device)] * bs\n",
    "    for index, x in enumerate(outputs):  # image index, image inference\n",
    "        x = x.transpose(0, -1)[xc[index]]  # confidence\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # matrix nx6 (box, confidence, cls)\n",
    "        box, cls = x.split((4, nc), 1)\n",
    "        box = xywh2xyxy(box)  # (cx, cy, w, h) to (x1, y1, x2, y2)\n",
    "        if nc > 1:\n",
    "            i, j = (cls > confidence_threshold).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, 4 + j, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = cls.max(1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > confidence_threshold]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence and remove excess boxes\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * max_wh  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes, scores\n",
    "        indices = torchvision.ops.nms(boxes, scores, iou_threshold)  # NMS\n",
    "        indices = indices[:max_det]  # limit detections\n",
    "\n",
    "        output[index] = x[indices]\n",
    "        if (time() - start) > limit:\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def smooth(y, f=0.1):\n",
    "    # Box filter of fraction f\n",
    "    nf = round(len(y) * f * 2) // 2 + 1  # number of filter elements (must be odd)\n",
    "    p = numpy.ones(nf // 2)  # ones padding\n",
    "    yp = numpy.concatenate((p * y[0], y, p * y[-1]), 0)  # y padded\n",
    "    return numpy.convolve(yp, numpy.ones(nf) / nf, mode='valid')  # y-smoothed\n",
    "\n",
    "\n",
    "def plot_pr_curve(px, py, ap, names, save_dir):\n",
    "    from matplotlib import pyplot\n",
    "    fig, ax = pyplot.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n",
    "    py = numpy.stack(py, axis=1)\n",
    "\n",
    "    if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n",
    "        for i, y in enumerate(py.T):\n",
    "            ax.plot(px, y, linewidth=1, label=f\"{names[i]} {ap[i, 0]:.3f}\")  # plot(recall, precision)\n",
    "    else:\n",
    "        ax.plot(px, py, linewidth=1, color=\"grey\")  # plot(recall, precision)\n",
    "\n",
    "    ax.plot(px, py.mean(1), linewidth=3, color=\"blue\", label=\"all classes %.3f mAP@0.5\" % ap[:, 0].mean())\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    ax.set_title(\"Precision-Recall Curve\")\n",
    "    fig.savefig(save_dir, dpi=250)\n",
    "    pyplot.close(fig)\n",
    "\n",
    "\n",
    "def plot_curve(px, py, names, save_dir, x_label=\"Confidence\", y_label=\"Metric\"):\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    figure, ax = pyplot.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n",
    "\n",
    "    if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n",
    "        for i, y in enumerate(py):\n",
    "            ax.plot(px, y, linewidth=1, label=f\"{names[i]}\")  # plot(confidence, metric)\n",
    "    else:\n",
    "        ax.plot(px, py.T, linewidth=1, color=\"grey\")  # plot(confidence, metric)\n",
    "\n",
    "    y = smooth(py.mean(0), f=0.05)\n",
    "    ax.plot(px, y, linewidth=3, color=\"blue\", label=f\"all classes {y.max():.3f} at {px[y.argmax()]:.3f}\")\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "    ax.set_title(f\"{y_label}-Confidence Curve\")\n",
    "    figure.savefig(save_dir, dpi=250)\n",
    "    pyplot.close(figure)\n",
    "\n",
    "\n",
    "def compute_ap(tp, conf, output, target, plot=False, names=(), eps=1E-16):\n",
    "    \"\"\"\n",
    "    Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:  True positives (nparray, nx1 or nx10).\n",
    "        conf:  Object-ness value from 0-1 (nparray).\n",
    "        output:  Predicted object classes (nparray).\n",
    "        target:  True object classes (nparray).\n",
    "    # Returns\n",
    "        The average precision\n",
    "    \"\"\"\n",
    "    # Sort by object-ness\n",
    "    i = numpy.argsort(-conf)\n",
    "    tp, conf, output = tp[i], conf[i], output[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes, nt = numpy.unique(target, return_counts=True)\n",
    "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    p = numpy.zeros((nc, 1000))\n",
    "    r = numpy.zeros((nc, 1000))\n",
    "    ap = numpy.zeros((nc, tp.shape[1]))\n",
    "    px, py = numpy.linspace(start=0, stop=1, num=1000), []  # for plotting\n",
    "    for ci, c in enumerate(unique_classes):\n",
    "        i = output == c\n",
    "        nl = nt[ci]  # number of labels\n",
    "        no = i.sum()  # number of outputs\n",
    "        if no == 0 or nl == 0:\n",
    "            continue\n",
    "\n",
    "        # Accumulate FPs and TPs\n",
    "        fpc = (1 - tp[i]).cumsum(0)\n",
    "        tpc = tp[i].cumsum(0)\n",
    "\n",
    "        # Recall\n",
    "        recall = tpc / (nl + eps)  # recall curve\n",
    "        # negative x, xp because xp decreases\n",
    "        r[ci] = numpy.interp(-px, -conf[i], recall[:, 0], left=0)\n",
    "\n",
    "        # Precision\n",
    "        precision = tpc / (tpc + fpc)  # precision curve\n",
    "        p[ci] = numpy.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
    "\n",
    "        # AP from recall-precision curve\n",
    "        for j in range(tp.shape[1]):\n",
    "            m_rec = numpy.concatenate(([0.0], recall[:, j], [1.0]))\n",
    "            m_pre = numpy.concatenate(([1.0], precision[:, j], [0.0]))\n",
    "\n",
    "            # Compute the precision envelope\n",
    "            m_pre = numpy.flip(numpy.maximum.accumulate(numpy.flip(m_pre)))\n",
    "\n",
    "            # Integrate area under curve\n",
    "            x = numpy.linspace(start=0, stop=1, num=101)  # 101-point interp (COCO)\n",
    "            ap[ci, j] = numpy.trapz(numpy.interp(x, m_rec, m_pre), x)  # integrate\n",
    "            if plot and j == 0:\n",
    "                py.append(numpy.interp(px, m_rec, m_pre))  # precision at mAP@0.5\n",
    "\n",
    "    # Compute F1 (harmonic mean of precision and recall)\n",
    "    f1 = 2 * p * r / (p + r + eps)\n",
    "    if plot:\n",
    "        names = dict(enumerate(names))  # to dict\n",
    "        names = [v for k, v in names.items() if k in unique_classes]  # list: only classes that have data\n",
    "        plot_pr_curve(px, py, ap, names, save_dir=\"./weights/PR_curve.png\")\n",
    "        plot_curve(px, f1, names, save_dir=\"./weights/F1_curve.png\", y_label=\"F1\")\n",
    "        plot_curve(px, p, names, save_dir=\"./weights/P_curve.png\", y_label=\"Precision\")\n",
    "        plot_curve(px, r, names, save_dir=\"./weights/R_curve.png\", y_label=\"Recall\")\n",
    "    i = smooth(f1.mean(0), 0.1).argmax()  # max F1 index\n",
    "    p, r, f1 = p[:, i], r[:, i], f1[:, i]\n",
    "    tp = (r * nt).round()  # true positives\n",
    "    fp = (tp / (p + eps) - tp).round()  # false positives\n",
    "    ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "    m_pre, m_rec = p.mean(), r.mean()\n",
    "    map50, mean_ap = ap50.mean(), ap.mean()\n",
    "    return tp, fp, m_pre, m_rec, map50, mean_ap\n",
    "\n",
    "\n",
    "def compute_iou(box1, box2, eps=1e-7):\n",
    "    # Returns Intersection over Union (IoU) of box1(1,4) to box2(n,4)\n",
    "\n",
    "    # Get the coordinates of bounding boxes\n",
    "    b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n",
    "    b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n",
    "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
    "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
    "\n",
    "    # Intersection area\n",
    "    inter = (b1_x2.minimum(b2_x2) - b1_x1.maximum(b2_x1)).clamp(0) * \\\n",
    "            (b1_y2.minimum(b2_y2) - b1_y1.maximum(b2_y1)).clamp(0)\n",
    "\n",
    "    # Union Area\n",
    "    union = w1 * h1 + w2 * h2 - inter + eps\n",
    "\n",
    "    # IoU\n",
    "    iou = inter / union\n",
    "    cw = b1_x2.maximum(b2_x2) - b1_x1.minimum(b2_x1)  # convex (smallest enclosing box) width\n",
    "    ch = b1_y2.maximum(b2_y2) - b1_y1.minimum(b2_y1)  # convex height\n",
    "    c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
    "    rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 + (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center dist ** 2\n",
    "    # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
    "    v = (4 / math.pi ** 2) * (torch.atan(w2 / h2) - torch.atan(w1 / h1)).pow(2)\n",
    "    with torch.no_grad():\n",
    "        alpha = v / (v - iou + (1 + eps))\n",
    "    return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
    "\n",
    "\n",
    "def strip_optimizer(filename):\n",
    "    x = torch.load(filename, map_location=\"cpu\")\n",
    "    x['model'].half()  # to FP16\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "    torch.save(x, f=filename)\n",
    "\n",
    "\n",
    "def clip_gradients(model, max_norm=10.0):\n",
    "    parameters = model.parameters()\n",
    "    torch.nn.utils.clip_grad_norm_(parameters, max_norm=max_norm)\n",
    "\n",
    "\n",
    "def load_weight(model, ckpt):\n",
    "    dst = model.state_dict()\n",
    "    src = torch.load(ckpt)['model'].float().cpu()\n",
    "\n",
    "    ckpt = {}\n",
    "    for k, v in src.state_dict().items():\n",
    "        if k in dst and v.shape == dst[k].shape:\n",
    "            ckpt[k] = v\n",
    "\n",
    "    model.load_state_dict(state_dict=ckpt, strict=False)\n",
    "    return model\n",
    "\n",
    "\n",
    "def set_params(model, decay):\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    norm = tuple(v for k, v in torch.nn.__dict__.items() if \"Norm\" in k)\n",
    "    for m in model.modules():\n",
    "        for n, p in m.named_parameters(recurse=0):\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            if n == \"bias\":  # bias (no decay)\n",
    "                p1.append(p)\n",
    "            elif n == \"weight\" and isinstance(m, norm):  # norm-weight (no decay)\n",
    "                p1.append(p)\n",
    "            else:\n",
    "                p2.append(p)  # weight (with decay)\n",
    "    return [{'params': p1, 'weight_decay': 0.00},\n",
    "            {'params': p2, 'weight_decay': decay}]\n",
    "\n",
    "\n",
    "def plot_lr(args, optimizer, scheduler, num_steps):\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    optimizer = copy.copy(optimizer)\n",
    "    scheduler = copy.copy(scheduler)\n",
    "\n",
    "    y = []\n",
    "    for epoch in range(args.epochs):\n",
    "        for i in range(num_steps):\n",
    "            step = i + num_steps * epoch\n",
    "            scheduler.step(step, optimizer)\n",
    "            y.append(optimizer.param_groups[0]['lr'])\n",
    "    pyplot.plot(y, '.-', label='LR')\n",
    "    pyplot.xlabel('step')\n",
    "    pyplot.ylabel('LR')\n",
    "    pyplot.grid()\n",
    "    pyplot.xlim(0, args.epochs * num_steps)\n",
    "    pyplot.ylim(0)\n",
    "    pyplot.savefig('./weights/lr.png', dpi=200)\n",
    "    pyplot.close()\n",
    "\n",
    "\n",
    "class CosineLR:\n",
    "    def __init__(self, args, params, num_steps):\n",
    "        max_lr = params['max_lr']\n",
    "        min_lr = params['min_lr']\n",
    "\n",
    "        warmup_steps = int(max(params['warmup_epochs'] * num_steps, 100))\n",
    "        decay_steps = int(args.epochs * num_steps - warmup_steps)\n",
    "\n",
    "        warmup_lr = numpy.linspace(min_lr, max_lr, int(warmup_steps))\n",
    "\n",
    "        decay_lr = []\n",
    "        for step in range(1, decay_steps + 1):\n",
    "            alpha = math.cos(math.pi * step / decay_steps)\n",
    "            decay_lr.append(min_lr + 0.5 * (max_lr - min_lr) * (1 + alpha))\n",
    "\n",
    "        self.total_lr = numpy.concatenate((warmup_lr, decay_lr))\n",
    "\n",
    "    def step(self, step, optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = self.total_lr[step]\n",
    "\n",
    "\n",
    "class LinearLR:\n",
    "    def __init__(self, args, params, num_steps):\n",
    "        max_lr = params['max_lr']\n",
    "        min_lr = params['min_lr']\n",
    "\n",
    "        warmup_steps = int(max(params['warmup_epochs'] * num_steps, 100))\n",
    "        decay_steps = int(args.epochs * num_steps - warmup_steps)\n",
    "\n",
    "        warmup_lr = numpy.linspace(min_lr, max_lr, int(warmup_steps), endpoint=False)\n",
    "        decay_lr = numpy.linspace(max_lr, min_lr, decay_steps)\n",
    "\n",
    "        self.total_lr = numpy.concatenate((warmup_lr, decay_lr))\n",
    "\n",
    "    def step(self, step, optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = self.total_lr[step]\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"\n",
    "    Updated Exponential Moving Average (EMA) from https://github.com/rwightman/pytorch-image-models\n",
    "    Keeps a moving average of everything in the model state_dict (parameters and buffers)\n",
    "    For EMA details see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, tau=2000, updates=0):\n",
    "        # Create EMA\n",
    "        self.ema = copy.deepcopy(model).eval()  # FP32 EMA\n",
    "        self.updates = updates  # number of EMA updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / tau))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "\n",
    "            msd = model.state_dict()  # model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1 - d) * msd[k].detach()\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.num = 0\n",
    "        self.sum = 0\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, v, n):\n",
    "        if not math.isnan(float(v)):\n",
    "            self.num = self.num + n\n",
    "            self.sum = self.sum + v * n\n",
    "            self.avg = self.sum / self.num\n",
    "\n",
    "\n",
    "class Assigner(torch.nn.Module):\n",
    "    def __init__(self, nc=80, top_k=10, alpha=0.5, beta=6.0, eps=1E-9):\n",
    "        super().__init__()\n",
    "        self.top_k = top_k\n",
    "        self.nc = nc\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.eps = eps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, pd_scores, pd_bboxes, anc_points, gt_labels, gt_bboxes, mask_gt):\n",
    "        batch_size = pd_scores.size(0)\n",
    "        num_max_boxes = gt_bboxes.size(1)\n",
    "\n",
    "        if num_max_boxes == 0:\n",
    "            device = gt_bboxes.device\n",
    "            return (torch.zeros_like(pd_bboxes).to(device),\n",
    "                    torch.zeros_like(pd_scores).to(device),\n",
    "                    torch.zeros_like(pd_scores[..., 0]).to(device))\n",
    "\n",
    "        num_anchors = anc_points.shape[0]\n",
    "        shape = gt_bboxes.shape\n",
    "        lt, rb = gt_bboxes.view(-1, 1, 4).chunk(2, 2)\n",
    "        mask_in_gts = torch.cat((anc_points[None] - lt, rb - anc_points[None]), dim=2)\n",
    "        mask_in_gts = mask_in_gts.view(shape[0], shape[1], num_anchors, -1).amin(3).gt_(self.eps)\n",
    "        na = pd_bboxes.shape[-2]\n",
    "        gt_mask = (mask_in_gts * mask_gt).bool()  # b, max_num_obj, h*w\n",
    "        overlaps = torch.zeros([batch_size, num_max_boxes, na], dtype=pd_bboxes.dtype, device=pd_bboxes.device)\n",
    "        bbox_scores = torch.zeros([batch_size, num_max_boxes, na], dtype=pd_scores.dtype, device=pd_scores.device)\n",
    "\n",
    "        ind = torch.zeros([2, batch_size, num_max_boxes], dtype=torch.long)  # 2, b, max_num_obj\n",
    "        ind[0] = torch.arange(end=batch_size).view(-1, 1).expand(-1, num_max_boxes)  # b, max_num_obj\n",
    "        ind[1] = gt_labels.squeeze(-1)  # b, max_num_obj\n",
    "        bbox_scores[gt_mask] = pd_scores[ind[0], :, ind[1]][gt_mask]  # b, max_num_obj, h*w\n",
    "\n",
    "        pd_boxes = pd_bboxes.unsqueeze(1).expand(-1, num_max_boxes, -1, -1)[gt_mask]\n",
    "        gt_boxes = gt_bboxes.unsqueeze(2).expand(-1, -1, na, -1)[gt_mask]\n",
    "        overlaps[gt_mask] = compute_iou(gt_boxes, pd_boxes).squeeze(-1).clamp_(0)\n",
    "\n",
    "        align_metric = bbox_scores.pow(self.alpha) * overlaps.pow(self.beta)\n",
    "\n",
    "        top_k_mask = mask_gt.expand(-1, -1, self.top_k).bool()\n",
    "        top_k_metrics, top_k_indices = torch.topk(align_metric, self.top_k, dim=-1, largest=True)\n",
    "        if top_k_mask is None:\n",
    "            top_k_mask = (top_k_metrics.max(-1, keepdim=True)[0] > self.eps).expand_as(top_k_indices)\n",
    "        top_k_indices.masked_fill_(~top_k_mask, 0)\n",
    "\n",
    "        mask_top_k = torch.zeros(align_metric.shape, dtype=torch.int8, device=top_k_indices.device)\n",
    "        ones = torch.ones_like(top_k_indices[:, :, :1], dtype=torch.int8, device=top_k_indices.device)\n",
    "        for k in range(self.top_k):\n",
    "            mask_top_k.scatter_add_(-1, top_k_indices[:, :, k:k + 1], ones)\n",
    "        mask_top_k.masked_fill_(mask_top_k > 1, 0)\n",
    "        mask_top_k = mask_top_k.to(align_metric.dtype)\n",
    "        mask_pos = mask_top_k * mask_in_gts * mask_gt\n",
    "\n",
    "        fg_mask = mask_pos.sum(-2)\n",
    "        if fg_mask.max() > 1:\n",
    "            mask_multi_gts = (fg_mask.unsqueeze(1) > 1).expand(-1, num_max_boxes, -1)\n",
    "            max_overlaps_idx = overlaps.argmax(1)\n",
    "\n",
    "            is_max_overlaps = torch.zeros(mask_pos.shape, dtype=mask_pos.dtype, device=mask_pos.device)\n",
    "            is_max_overlaps.scatter_(1, max_overlaps_idx.unsqueeze(1), 1)\n",
    "\n",
    "            mask_pos = torch.where(mask_multi_gts, is_max_overlaps, mask_pos).float()\n",
    "            fg_mask = mask_pos.sum(-2)\n",
    "        target_gt_idx = mask_pos.argmax(-2)\n",
    "\n",
    "        # Assigned target\n",
    "        index = torch.arange(end=batch_size, dtype=torch.int64, device=gt_labels.device)[..., None]\n",
    "        target_index = target_gt_idx + index * num_max_boxes\n",
    "        target_labels = gt_labels.long().flatten()[target_index]\n",
    "\n",
    "        target_bboxes = gt_bboxes.view(-1, gt_bboxes.shape[-1])[target_index]\n",
    "\n",
    "        # Assigned target scores\n",
    "        target_labels.clamp_(0)\n",
    "\n",
    "        target_scores = torch.zeros((target_labels.shape[0], target_labels.shape[1], self.nc),\n",
    "                                    dtype=torch.int64,\n",
    "                                    device=target_labels.device)\n",
    "        target_scores.scatter_(2, target_labels.unsqueeze(-1), 1)\n",
    "\n",
    "        fg_scores_mask = fg_mask[:, :, None].repeat(1, 1, self.nc)\n",
    "        target_scores = torch.where(fg_scores_mask > 0, target_scores, 0)\n",
    "\n",
    "        # Normalize\n",
    "        align_metric *= mask_pos\n",
    "        pos_align_metrics = align_metric.amax(dim=-1, keepdim=True)\n",
    "        pos_overlaps = (overlaps * mask_pos).amax(dim=-1, keepdim=True)\n",
    "        norm_align_metric = (align_metric * pos_overlaps / (pos_align_metrics + self.eps)).amax(-2).unsqueeze(-1)\n",
    "        target_scores = target_scores * norm_align_metric\n",
    "\n",
    "        return target_bboxes, target_scores, fg_mask.bool()\n",
    "\n",
    "\n",
    "class QFL(torch.nn.Module):\n",
    "    def __init__(self, beta=2.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        bce_loss = self.bce_loss(outputs, targets)\n",
    "        return torch.pow(torch.abs(targets - outputs.sigmoid()), self.beta) * bce_loss\n",
    "\n",
    "\n",
    "class VFL(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.00, iou_weighted=True):\n",
    "        super().__init__()\n",
    "        assert alpha >= 0.0\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.iou_weighted = iou_weighted\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        assert outputs.size() == targets.size()\n",
    "        targets = targets.type_as(outputs)\n",
    "\n",
    "        if self.iou_weighted:\n",
    "            focal_weight = targets * (targets > 0.0).float() + \\\n",
    "                           self.alpha * (outputs.sigmoid() - targets).abs().pow(self.gamma) * \\\n",
    "                           (targets <= 0.0).float()\n",
    "\n",
    "        else:\n",
    "            focal_weight = (targets > 0.0).float() + \\\n",
    "                           self.alpha * (outputs.sigmoid() - targets).abs().pow(self.gamma) * \\\n",
    "                           (targets <= 0.0).float()\n",
    "\n",
    "        return self.bce_loss(outputs, targets) * focal_weight\n",
    "\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=1.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        loss = self.bce_loss(outputs, targets)\n",
    "\n",
    "        if self.alpha > 0:\n",
    "            alpha_factor = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "            loss *= alpha_factor\n",
    "\n",
    "        if self.gamma > 0:\n",
    "            outputs_sigmoid = outputs.sigmoid()\n",
    "            p_t = targets * outputs_sigmoid + (1 - targets) * (1 - outputs_sigmoid)\n",
    "            gamma_factor = (1.0 - p_t) ** self.gamma\n",
    "            loss *= gamma_factor\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BoxLoss(torch.nn.Module):\n",
    "    def __init__(self, dfl_ch):\n",
    "        super().__init__()\n",
    "        self.dfl_ch = dfl_ch\n",
    "\n",
    "    def forward(self, pred_dist, pred_bboxes, anchor_points, target_bboxes, target_scores, target_scores_sum, fg_mask):\n",
    "        # IoU loss\n",
    "        weight = torch.masked_select(target_scores.sum(-1), fg_mask).unsqueeze(-1)\n",
    "        iou = compute_iou(pred_bboxes[fg_mask], target_bboxes[fg_mask])\n",
    "        loss_box = ((1.0 - iou) * weight).sum() / target_scores_sum\n",
    "\n",
    "        # DFL loss\n",
    "        a, b = target_bboxes.chunk(2, -1)\n",
    "        target = torch.cat((anchor_points - a, b - anchor_points), -1)\n",
    "        target = target.clamp(0, self.dfl_ch - 0.01)\n",
    "        loss_dfl = self.df_loss(pred_dist[fg_mask].view(-1, self.dfl_ch + 1), target[fg_mask])\n",
    "        loss_dfl = (loss_dfl * weight).sum() / target_scores_sum\n",
    "\n",
    "        return loss_box, loss_dfl\n",
    "\n",
    "    @staticmethod\n",
    "    def df_loss(pred_dist, target):\n",
    "        # Distribution Focal Loss (DFL)\n",
    "        # https://ieeexplore.ieee.org/document/9792391\n",
    "        tl = target.long()  # target left\n",
    "        tr = tl + 1  # target right\n",
    "        wl = tr - target  # weight left\n",
    "        wr = 1 - wl  # weight right\n",
    "        left_loss = cross_entropy(pred_dist, tl.view(-1), reduction='none').view(tl.shape)\n",
    "        right_loss = cross_entropy(pred_dist, tr.view(-1), reduction='none').view(tl.shape)\n",
    "        return (left_loss * wl + right_loss * wr).mean(-1, keepdim=True)\n",
    "\n",
    "\n",
    "class ComputeLoss:\n",
    "    def __init__(self, model, params):\n",
    "        if hasattr(model, 'module'):\n",
    "            model = model.module\n",
    "\n",
    "        device = next(model.parameters()).device\n",
    "\n",
    "        m = model[22]  # Head() module\n",
    "\n",
    "        self.params = params\n",
    "        self.stride = m.stride\n",
    "        self.nc = m.nc\n",
    "        self.no = m.no\n",
    "        self.reg_max = m.ch\n",
    "        self.device = device\n",
    "\n",
    "        self.assigner = Assigner(self.nc)\n",
    "        self.box_loss = BoxLoss(m.ch - 1).to(device)\n",
    "        self.cls_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "        self.project = torch.arange(m.ch, dtype=torch.float, device=device)\n",
    "\n",
    "    def box_decode(self, anchor_points, pred_dist):\n",
    "        b, a, c = pred_dist.shape\n",
    "        pred_dist = pred_dist.view(b, a, 4, c // 4)\n",
    "        pred_dist = pred_dist.softmax(3)\n",
    "        pred_dist = pred_dist.matmul(self.project.type(pred_dist.dtype))\n",
    "        lt, rb = pred_dist.chunk(2, -1)\n",
    "        x1y1 = anchor_points - lt\n",
    "        x2y2 = anchor_points + rb\n",
    "        return torch.cat(tensors=(x1y1, x2y2), dim=-1)\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        x = torch.cat([i.view(outputs[0].shape[0], self.no, -1) for i in outputs], dim=2)\n",
    "        boxes, scores = x.split(split_size=(self.reg_max * 4, self.nc), dim=1)\n",
    "\n",
    "        boxes = boxes.permute(0, 2, 1).contiguous()\n",
    "        scores = scores.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        data_type = scores.dtype\n",
    "        batch_size = scores.shape[0]\n",
    "        input_size = torch.tensor(outputs[0].shape[2:], device=self.device, dtype=data_type) * self.stride[0]\n",
    "        anchor_points, stride_tensor = make_anchors(outputs, self.stride, offset=0.5)\n",
    "\n",
    "        idx = targets['idx'].view(-1, 1)\n",
    "        cls = targets['cls'].view(-1, 1)\n",
    "        box = targets['box']\n",
    "\n",
    "        targets = torch.cat(tensors=(idx, cls, box), dim=1).to(self.device)\n",
    "        if targets.shape[0] == 0:\n",
    "            gt = torch.zeros(batch_size, 0, 5, device=self.device)\n",
    "        else:\n",
    "            i = targets[:, 0]\n",
    "            _, counts = i.unique(return_counts=True)\n",
    "            counts = counts.to(dtype=torch.int32)\n",
    "            gt = torch.zeros(batch_size, counts.max(), 5, device=self.device)\n",
    "            for j in range(batch_size):\n",
    "                matches = i == j\n",
    "                n = matches.sum()\n",
    "                if n:\n",
    "                    gt[j, :n] = targets[matches, 1:]\n",
    "            x = gt[..., 1:5].mul_(input_size[[1, 0, 1, 0]])\n",
    "            y = torch.empty_like(x)\n",
    "            dw = x[..., 2] / 2  # half-width\n",
    "            dh = x[..., 3] / 2  # half-height\n",
    "            y[..., 0] = x[..., 0] - dw  # top left x\n",
    "            y[..., 1] = x[..., 1] - dh  # top left y\n",
    "            y[..., 2] = x[..., 0] + dw  # bottom right x\n",
    "            y[..., 3] = x[..., 1] + dh  # bottom right y\n",
    "            gt[..., 1:5] = y\n",
    "\n",
    "        target_labels, target_bboxes = gt.split(split_size=(1, 4), dim=2)\n",
    "        target_mask = target_bboxes.sum(2, keepdim=True).gt_(0)\n",
    "\n",
    "        decoded_boxes = self.box_decode(anchor_points, boxes)\n",
    "        assigned_targets = self.assigner(scores.detach().sigmoid(),\n",
    "                                         (decoded_boxes.detach() * stride_tensor).type(target_bboxes.dtype),\n",
    "                                         anchor_points * stride_tensor, target_labels, target_bboxes, target_mask)\n",
    "        target_bboxes, target_scores, fg_mask = assigned_targets\n",
    "\n",
    "        target_scores_sum = max(target_scores.sum(), 1)\n",
    "\n",
    "        loss_cls = self.cls_loss(scores, target_scores.to(data_type)).sum() / target_scores_sum  # BCE\n",
    "\n",
    "        # Box loss\n",
    "        loss_box = torch.zeros(1, device=self.device)\n",
    "        loss_dfl = torch.zeros(1, device=self.device)\n",
    "        if fg_mask.sum():\n",
    "            target_bboxes /= stride_tensor\n",
    "            loss_box, loss_dfl = self.box_loss(boxes,\n",
    "                                               decoded_boxes,\n",
    "                                               anchor_points,\n",
    "                                               target_bboxes,\n",
    "                                               target_scores,\n",
    "                                               target_scores_sum, fg_mask)\n",
    "\n",
    "        loss_box *= self.params['box']  # box gain\n",
    "        loss_cls *= self.params['cls']  # cls gain\n",
    "        loss_dfl *= self.params['dfl']  # dfl gain\n",
    "\n",
    "        return loss_box, loss_cls, loss_dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:06:22.034919Z",
     "iopub.status.busy": "2025-09-16T14:06:22.034616Z",
     "iopub.status.idle": "2025-09-16T14:06:22.096795Z",
     "shell.execute_reply": "2025-09-16T14:06:22.095885Z",
     "shell.execute_reply.started": "2025-09-16T14:06:22.034894Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.03)\n",
    "        self.act = nn.SiLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "    \n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.cv1 = Conv(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.cv2 = Conv(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.add = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        x = self.cv1(x)\n",
    "        x = self.cv2(x)\n",
    "        if self.add:\n",
    "            x += x_in\n",
    "        return x\n",
    "        \n",
    "\n",
    "class C2f(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_bottlenecks, shortcut=True):\n",
    "        super().__init__()\n",
    "        self.mid_channels = out_channels // 2\n",
    "        self.num_bottlenecks = num_bottlenecks\n",
    "        self.cv1 = Conv(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.cv2 = Conv((num_bottlenecks+2)*out_channels//2, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.m = nn.ModuleList([Bottleneck(self.mid_channels, self.mid_channels, shortcut) for _ in range(num_bottlenecks)]) # n bottlenecks\n",
    "        self.add = shortcut\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "        x1, x2 = x[:, :x.shape[1]//2, :, :], x[:, x.shape[1]//2:, :, :]\n",
    "        outputs = [x1, x2] # x1 is fed to the bottlenecks\n",
    "\n",
    "        for i in range(self.num_bottlenecks):\n",
    "            x1 = self.m[i](x1)\n",
    "            outputs.insert(0, x1)\n",
    "        \n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        out = self.cv2(outputs)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class SPPF(nn.Module): # EXPLORE WHY!!!!\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5): #kernel_size = size of maxpool\n",
    "        super().__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.cv1 = Conv(in_channels, hidden_channels, kernel_size=1, stride=1, padding=0) # WHY???\n",
    "        self.pool = nn.MaxPool2d(kernel_size=kernel_size, stride=1, padding=kernel_size//2, dilation=1, ceil_mode=False) # WHY???\n",
    "        self.cv2 = Conv(4*hidden_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cv1(x)\n",
    "\n",
    "        y1 = self.pool(x)\n",
    "        y2 = self.pool(y1)\n",
    "        y3 = self.pool(y2)\n",
    "\n",
    "        y = torch.cat([x,y1,y2,y3], dim=1)\n",
    "        \n",
    "        y = self.cv2(y)\n",
    "        return y\n",
    "\n",
    "    \n",
    "def yolo_params(version): # return d,w,r\n",
    "    if version == 'n':\n",
    "        return (1/3,1/4,2.0)\n",
    "    elif version == 's':\n",
    "        return (1/3,1/2,2.0)\n",
    "    elif version == 'm':\n",
    "        return (2/3,3/4,1.5)\n",
    "    elif version == 'l':\n",
    "        return (1.0,1.0,1.0)\n",
    "    elif version == 'x':\n",
    "        return (1.0, 1.25, 1.0)\n",
    "# [b, c, h, w]\n",
    "# P3: [B, 256, 80, 80] → stride 8\n",
    "# P4: [B, 512, 40, 40] → stride 16\n",
    "# P5: [B, 1024, 20, 20] → stride 32\n",
    "\n",
    "\n",
    "# DFL (“Distribution Focal Loss”) stores probabilities over reg_max(ch) bins for each side (l,t,r,b).\n",
    "# To decode those distributions into distances, you take the expected value over bins:\n",
    "\n",
    "# softmax over bins → probabilities\n",
    "\n",
    "# dot product with [0,1,2,…,reg_max−1] → expected bin index\n",
    "\n",
    "# (later you multiply by stride to get pixels)\n",
    "\n",
    "class Concat(nn.Module):\n",
    "    def __init__(self, dim=1): \n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "    def forward(self, xs):       # xs is a tuple/list of tensors\n",
    "        return torch.cat(xs, self.dim)\n",
    "\n",
    "\n",
    "\n",
    "class DFL(nn.Module):\n",
    "    def __init__(self, ch=16):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.conv = nn.Conv2d(in_channels=ch, out_channels=1, kernel_size=1, stride=1, padding=0, bias=False).requires_grad_(False)\n",
    "\n",
    "        x = torch.arange(self.ch, dtype=torch.float).view(1, self.ch, 1, 1)\n",
    "        self.conv.weight.data.copy_(x)\n",
    "    \n",
    "    def forward(self, x): # x = [B, C_in, c]\n",
    "        b, c, a = x.shape # b = B  c = C_in = 4*ch  a = c\n",
    "        x = x.view(b, 4, self.ch, a).transpose(1, 2)  # [B, ch(values), 4, c]\n",
    "\n",
    "        x = x.softmax(1)  # [B, ch(softmax values), 4, c]\n",
    "        x = self.conv(x)  # [B, 1, 4, c]\n",
    "        return x.view(b, 4, a)  # [B, 4, c] so it returns the l,t,r,b values(in bin) for every batch (we don't need the out channel of conv)\n",
    "\n",
    "\n",
    "class Detect(nn.Module):\n",
    "    def __init__(self, version, ch=16, nc=4):\n",
    "        super().__init__()\n",
    "        self.ch=ch                          # dfl channels\n",
    "        self.coordinates=self.ch*4          # number of bounding boxes coordinates\n",
    "        self.nc=nc                 # 4 for our dataset\n",
    "        self.no=self.coordinates+self.nc    # num of outputs per anchor box\n",
    "        self.stride=torch.zeros(0)          # strides computed during build\n",
    "        d,w,r = yolo_params(version=version)\n",
    "\n",
    "        self.cv2=nn.ModuleList([\n",
    "            # for box\n",
    "            nn.Sequential(Conv(int(256*w), self.coordinates, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.coordinates, self.coordinates, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.coordinates, self.coordinates, kernel_size=1, stride=1, padding=0)),\n",
    "            \n",
    "            nn.Sequential(Conv(int(512*w), self.coordinates, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.coordinates, self.coordinates, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.coordinates, self.coordinates, kernel_size=1, stride=1, padding=0)),\n",
    "            \n",
    "            nn.Sequential(Conv(int(512*w*r), self.coordinates, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.coordinates, self.coordinates, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.coordinates, self.coordinates, kernel_size=1, stride=1, padding=0)),\n",
    "        ])\n",
    "\n",
    "        # for classification\n",
    "        self.cv3=nn.ModuleList([\n",
    "            nn.Sequential(Conv(int(256*w), self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.nc, self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)),\n",
    "            \n",
    "            nn.Sequential(Conv(int(512*w), self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.nc, self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)),\n",
    "            \n",
    "            nn.Sequential(Conv(int(512*w*r), self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          Conv(self.nc, self.nc, kernel_size=3, stride=1, padding=1),\n",
    "                          nn.Conv2d(self.nc, self.nc, kernel_size=1, stride=1, padding=0)),\n",
    "        ])\n",
    "\n",
    "        # dfl\n",
    "        self.dfl = DFL()\n",
    "\n",
    "    def forward(self, x): # x = (out1,out2,out3), outx = [B, chx, wx, hx]\n",
    "        for i in range(len(self.cv2)):\n",
    "            box = self.cv2[i](x[i])     # [b, num_coordinates, w, h]\n",
    "            cls = self.cv3[i](x[i])     # [b, num_classes, w, h]\n",
    "            x[i] = torch.cat((box, cls), dim=1) # [b, num_coordinates+num_classes, w, h] \n",
    "\n",
    "        # in training no dfl output\n",
    "        if self.training:\n",
    "            return x    # [3,b,num_coordinates+num_classes,w,h]\n",
    "        \n",
    "        # in inference, dfl produces refined bounding box coordinates\n",
    "        anchors, strides = (i.transpose(0, 1) for i in self.make_anchors(x, self.stride))\n",
    "\n",
    "        # concatinate predictions from all detection layers\n",
    "        x = torch.cat([i.view(x[0].shape[0], self.no, -1) for i in x], dim=2)  # [b, 4*self.ch + self.nc, sum_i(h[i]w[i])]\n",
    "\n",
    "        # split out predictions for box and cls\n",
    "        #       box=[b,4*self.ch,sum_i(h[i]w[i])]\n",
    "        #       cls=[b,self.nc,sum_i(h[i]w[i])]\n",
    "        box, cls = x.split(split_size=(4*self.ch, self.nc), dim=1)\n",
    "\n",
    "        a, b = self.dfl(box).chunk(2, 1)    # a=b=[b,2*self.ch,sum_i(h[i]w[i])]\n",
    "        a = anchors.unsqueeze(0) - a\n",
    "        b = anchors.unsqueeze(0) + b\n",
    "        box = torch.cat(tensors=((a + b) / 2, b - a), dim=1)\n",
    "\n",
    "        return torch.cat(tensors=(box * strides, cls.sigmoid()), dim=1)\n",
    "    \n",
    "    def make_anchors(self, x, strides, offset=0.5):\n",
    "        # x = list of feature maps: x = [x[0],...,x[N-1]], N=num_detection_heads=3\n",
    "        # each having shape [b, ch, w, h]\n",
    "        # each feature map x[i] gives output output[i] = w*h anchor coordinates + w*h stride values\n",
    "        # strides = coefficient of how much feature map is reduced compared to the original image\n",
    "        assert x is not None\n",
    "        anchor_tensor, stride_tensor = [], []\n",
    "        dtype, device = x[0].dtype, x[0].device\n",
    "        for i, stride in enumerate(strides):\n",
    "            _, _, h, w = x[i].shape\n",
    "            sx = torch.arange(end=w, device=device, dtype=dtype) + offset \n",
    "            sy = torch.arange(end=h, device=device, dtype=dtype) + offset\n",
    "            sy, sx = torch.meshgrid(sy, sx)\n",
    "            anchor_tensor.append(torch.stack((sx, sy), -1).view(-1, 2))\n",
    "            stride_tensor.append(torch.full((h * w, 1), stride, dtype=dtype, device=device))\n",
    "        return torch.cat(anchor_tensor), torch.cat(stride_tensor)\n",
    "    \n",
    "\n",
    "\n",
    "class YOLO(nn.Module):\n",
    "    def __init__(self, task=None, verbose=False, version='n', in_channels=3):\n",
    "        super().__init__()\n",
    "        d, w, r = yolo_params(version)\n",
    "        # self.stride=torch.zeros(0)\n",
    "        self.predictor = None  # reuse predictor\n",
    "        self.model = None  # model object\n",
    "        self.trainer = None  # trainer object\n",
    "        self.ckpt = {}  # if loaded from *.pt\n",
    "        self.cfg = None  # if loaded from *.yaml\n",
    "        self.ckpt_path = None\n",
    "        self.overrides = {}  # overrides for trainer object\n",
    "        self.metrics = None  # validation/training metrics\n",
    "        self.session = None  # HUB session\n",
    "        self.task = task  # task type\n",
    "        self.model_name = None  # model name\n",
    "        self.model = nn.ModuleList([\n",
    "            # backbone\n",
    "            Conv(in_channels, int(64*w), kernel_size=3, stride=2, padding=1),              #0\n",
    "            Conv(int(64*w), int(128*w), kernel_size=3, stride=2, padding=1),               #1\n",
    "            C2f(int(128*w), int(128*w), num_bottlenecks=int(3*d), shortcut=True),          #2\n",
    "            Conv(int(128*w), int(256*w), kernel_size=3, stride=2, padding=1),              #3\n",
    "            C2f(int(256*w), int(256*w), num_bottlenecks=int(6*d), shortcut=True),          #4\n",
    "            Conv(int(256*w), int(512*w), kernel_size=3, stride=2, padding=1),              #5\n",
    "            C2f(int(512*w), int(512*w), num_bottlenecks=int(6*d), shortcut=True),          #6\n",
    "            Conv(int(512*w), int(512*w*r), kernel_size=3, stride=2, padding=1),            #7\n",
    "            C2f(int(512*w*r), int(512*w*r), num_bottlenecks=int(3*d), shortcut=True),      #8\n",
    "            SPPF(int(512*w*r), int(512*w*r)),                                              #9\n",
    "\n",
    "            # neck\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),                                   #10\n",
    "            Concat(),                                                                      #11\n",
    "            C2f(int(512*w*(1+r)), int(512*w), num_bottlenecks=int(3*d), shortcut=False),   #12\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),                                   #13\n",
    "            Concat(),                                                                      #14\n",
    "            C2f(int(768*w), int(256*w), num_bottlenecks=int(3*d), shortcut=False),         #15\n",
    "            Conv(int(256*w), int(256*w), kernel_size=3, stride=2, padding=1),              #16\n",
    "            Concat(),                                                                      #17\n",
    "            C2f(int(768*w), int(512*w), num_bottlenecks=int(3*d), shortcut=False),         #18\n",
    "            Conv(int(512*w), int(512*w), kernel_size=3, stride=2, padding=1),              #19\n",
    "            Concat(),                                                                      #20\n",
    "            C2f(int(512*w*(1+r)), int(512*w*r), num_bottlenecks=int(3*d), shortcut=False), #21\n",
    "\n",
    "            # head\n",
    "            Detect(version=version),                                                       #22\n",
    "        ])\n",
    "        # Delete super().training for accessing self.model.training\n",
    "        del self.training\n",
    "\n",
    "    def forward(self, x):\n",
    "        # backbone forward\n",
    "        x = self.model[0](x)\n",
    "        x = self.model[1](x)\n",
    "        x = self.model[2](x)\n",
    "        x = self.model[3](x)\n",
    "        out1 = self.model[4](x) # for concat\n",
    "        x = self.model[5](out1)\n",
    "        out2 = self.model[6](x) # for concat\n",
    "        x = self.model[7](out2)\n",
    "        x = self.model[8](x)\n",
    "        out3 = self.model[9](x)\n",
    "\n",
    "        # neck forward\n",
    "        res_1 = out3 # for residual connection\n",
    "        x = self.model[10](out3)\n",
    "        x = self.model[11]((x, out2))\n",
    "        res_2 = self.model[12](x) # for concat\n",
    "        x = self.model[13](res_2)\n",
    "        x = self.model[14]((x, out1))\n",
    "        x1 = self.model[15](x) # for detect\n",
    "        x = self.model[16](x1)\n",
    "        x = self.model[17]((x, res_2))\n",
    "        x2 = self.model[18](x) # for detect\n",
    "        x = self.model[19](x2)\n",
    "        x = self.model[20]((x, res_1))\n",
    "        x3 = self.model[21](x) # for detect\n",
    "\n",
    "        return self.model[22]([x1,x2,x3])\n",
    "\n",
    "    def train(self, mode: bool | None = None, **ultra_kwargs):\n",
    "        # 1) Pure PyTorch toggle if mode is given and no kwargs\n",
    "        if (mode is not None) and (not ultra_kwargs):\n",
    "            return super().train(mode)\n",
    "\n",
    "        # 2) Ultralytics-style API when kwargs are provided\n",
    "        if ultra_kwargs:\n",
    "            return self._ultra_train(**ultra_kwargs)\n",
    "\n",
    "        # 3) Default: behave like model.train(True)\n",
    "        return super().train(True)\n",
    "\n",
    "    # ---- the high-level trainer ----\n",
    "    def _ultra_train(\n",
    "        self,\n",
    "        data: str,\n",
    "        epochs: int = 100,\n",
    "        imgsz: int = 640,\n",
    "        batch: int = 16,\n",
    "        name: str = \"exp\",\n",
    "        project: str = \"runs/train\",\n",
    "        device: int | str = 0,\n",
    "        patience: int = 50,\n",
    "        cos_lr: bool = True,\n",
    "        lr: float = 5e-4,\n",
    "        weight_decay: float = 5e-4,\n",
    "        workers: int = 8,\n",
    "        amp: bool = True,\n",
    "        grad_clip: float | None = 10.0,\n",
    "    ):\n",
    "        # device\n",
    "        if isinstance(device, (int, str)) and str(device).isdigit() and torch.cuda.is_available():\n",
    "            device = torch.device(f\"cuda:{device}\")\n",
    "        else:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(device)\n",
    "\n",
    "        # I/O\n",
    "        run_dir = os.path.join(project, name)\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        best_path, last_path = os.path.join(run_dir, \"best.pt\"), os.path.join(run_dir, \"last.pt\")\n",
    "\n",
    "        # data.yaml (expects keys: train, val, names/nc)\n",
    "        with open(data, \"r\") as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "        nc = cfg.get(\"nc\", len(cfg[\"names\"]))\n",
    "        names = cfg.get(\"names\")\n",
    "        \n",
    "\n",
    "        # build loaders (plug in YOUR dataset + collate_fn)\n",
    "        train_ds = YoloDataset(Path(cfg[\"train\"]), imgsz=imgsz, names=names)           # <-- your class\n",
    "        val_path = cfg.get(\"val\")\n",
    "        val_ds   = YoloDataset(Path(val_path), imgsz=imgsz, names=names) if val_path else None\n",
    "\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch, shuffle=True,\n",
    "                                  num_workers=workers, pin_memory=True,\n",
    "                                  collate_fn=train_ds.collate_fn)\n",
    "        val_loader = None\n",
    "        if val_ds:\n",
    "            val_loader = DataLoader(val_ds, batch_size=batch, shuffle=False,\n",
    "                                    num_workers=max(1, workers//2), pin_memory=True,\n",
    "                                    collate_fn=val_ds.collate_fn, drop_last=False)\n",
    "\n",
    "        def _gpu_mem_str(device):\n",
    "            if device.type == \"cuda\":\n",
    "                try:\n",
    "                    m = torch.cuda.memory_reserved(device.index if device.index is not None else 0)\n",
    "                except Exception:\n",
    "                    m = torch.cuda.max_memory_allocated()\n",
    "                return f\"{m / (1024**3):>7.2f}G\"\n",
    "            return f\"{0.0:>7.2f}G\"\n",
    "        \n",
    "        class _EMAval:\n",
    "            def __init__(self, beta=0.9): self.b=beta; self.v=None\n",
    "            def upd(self,x): x=float(x); self.v=x if self.v is None else self.b*self.v+(1-self.b)*x; return self.v\n",
    "        \n",
    "        def _epoch_header():\n",
    "            print(f\"{'Epoch':>10} {'GPU_mem':>9} {'box_loss':>9} {'cls_loss':>9} {'dfl_loss':>9} {'Instances':>10} {'Size':>10}\")\n",
    "\n",
    "        # loss, opt, sched\n",
    "        criterion = ComputeLoss(self.model, {\"nc\": nc, \"names\": names})          # <-- your loss\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = (torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "                     if cos_lr else torch.optim.lr_scheduler.MultiStepLR(optimizer, [int(0.8*epochs)], gamma=0.1))\n",
    "        scaler = torch.amp.GradScaler(\"cuda\", enabled=(amp and device.type == \"cuda\"))\n",
    "\n",
    "        # loop with early stopping on val loss\n",
    "        best_val = float(\"inf\"); bad_epochs = 0\n",
    "        for epoch in range(epochs):\n",
    "            super().train(True)  # PyTorch training mode\n",
    "            _epoch_header()\n",
    "            eb, ec, ed = _EMAval(), _EMAval(), _EMAval()\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            pbar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True, ncols=120,\n",
    "                bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\")\n",
    "            \n",
    "            for i, (imgs, targets) in pbar:\n",
    "                imgs = imgs.to(device, non_blocking=True).float() / 255.0\n",
    "                instances = int(targets[\"cls\"].numel())\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                with torch.autocast(\"cuda\", dtype=torch.float16, enabled=(amp and device.type == \"cuda\")):\n",
    "                    outputs = self(imgs)  # training path → 3 feature maps\n",
    "                    box_loss, cls_loss, dfl_loss = criterion(outputs, targets)\n",
    "                    loss = box_loss + cls_loss + dfl_loss\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                if grad_clip is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    nn.utils.clip_grad_norm_(self.parameters(), grad_clip)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                # running EMAs for smooth prints\n",
    "                b, c, d = eb.upd(box_loss.item()), ec.upd(cls_loss.item()), ed.upd(dfl_loss.item())\n",
    "                desc = f\"{epoch+1:>7}/{epochs:<3} {_gpu_mem_str(device)} {b:>9.3f} {c:>9.3f} {d:>9.3f} {instances:>10d} {imgsz:>10d}:\"\n",
    "                pbar.set_description_str(desc)\n",
    "    \n",
    "            \n",
    "            # for imgs, targets in train_loader:\n",
    "            #     imgs = imgs.to(device, non_blocking=True).float() / 255.0\n",
    "            #     optimizer.zero_grad(set_to_none=True)\n",
    "            #     with torch.autocast(\"cuda\", dtype=torch.float16, enabled=(amp and device.type == \"cuda\")):\n",
    "            #         outputs = self(imgs)  # your forward: returns list[3] in training mode\n",
    "            #         box_loss, cls_loss, dfl_loss = criterion(outputs, targets)\n",
    "            #         loss = box_loss + cls_loss + dfl_loss\n",
    "\n",
    "            #     scaler.scale(loss).backward()\n",
    "            #     if grad_clip is not None:\n",
    "            #         scaler.unscale_(optimizer)\n",
    "            #         nn.utils.clip_grad_norm_(self.parameters(), grad_clip)\n",
    "            #     scaler.step(optimizer)\n",
    "            #     scaler.update()\n",
    "            #     epoch_loss += loss.item()\n",
    "\n",
    "            scheduler.step()\n",
    "            train_loss = epoch_loss / max(1, len(train_loader))\n",
    "\n",
    "            # validation\n",
    "            val_loss = train_loss\n",
    "            if val_loader is not None:\n",
    "                super().eval()\n",
    "                # header above the val bar\n",
    "                print((\" \" * 17) + \"Class     Images  Instances      Box(P          R      mAP50  mAP50-95):\", end=\" \")\n",
    "                \n",
    "                vbar = tqdm(enumerate(val_loader), total=len(val_loader), leave=True, ncols=120,\n",
    "                    bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\")\n",
    "\n",
    "                iouv = torch.linspace(0.5, 0.95, 10, device=device)\n",
    "                tp_list, conf_list, pcls_list, tcls_list = [], [], [], []\n",
    "                total = 0.0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for _, (imgs, targets) in vbar:\n",
    "                        imgs = imgs.to(device, non_blocking=True).float() / 255.0\n",
    "                        outputs = self(imgs)  # inference path → [B, 4+nc, N]\n",
    "                        vb, vc, vd = criterion(outputs, targets)\n",
    "                        total += (vb + vc + vd).item()\n",
    "        \n",
    "                        # NMS → list of [ni,6] (x1,y1,x2,y2,conf,cls)\n",
    "                        preds = non_max_suppression(outputs, confidence_threshold=0.001, iou_threshold=0.7)\n",
    "        \n",
    "                        B, _, H, W = imgs.shape\n",
    "                        for b in range(B):\n",
    "                            p = preds[b]\n",
    "                            m = (targets[\"idx\"] == b)\n",
    "                            if m.any():\n",
    "                                gt_cls = targets[\"cls\"][m].to(device)\n",
    "                                gt_box = xywhn2xyxy(targets[\"box\"][m].to(device), W, H)\n",
    "                                t = torch.zeros((gt_cls.numel(), 5), device=device)\n",
    "                                t[:, 0] = gt_cls; t[:, 1:] = gt_box\n",
    "                            else:\n",
    "                                t = torch.zeros((0, 5), device=device)\n",
    "        \n",
    "                            if p.numel() == 0:\n",
    "                                if t.numel():\n",
    "                                    tcls_list.append(t[:, 0].cpu().numpy())\n",
    "                                continue\n",
    "        \n",
    "                            correct = compute_metric(p, t, iouv)  # [n_det, 10] bool\n",
    "                            tp_list.append(correct.cpu().numpy().astype(int))\n",
    "                            conf_list.append(p[:, 4].cpu().numpy())\n",
    "                            pcls_list.append(p[:, 5].cpu().numpy())\n",
    "                            if t.numel():\n",
    "                                tcls_list.append(t[:, 0].cpu().numpy())\n",
    "        \n",
    "                val_loss = total / max(1, len(val_loader))\n",
    "        \n",
    "                # reduce metrics across dataset\n",
    "                if conf_list:\n",
    "                    tp   = np.concatenate(tp_list, 0)\n",
    "                    conf = np.concatenate(conf_list, 0)\n",
    "                    pcls = np.concatenate(pcls_list, 0).astype(int)\n",
    "                    tcls = np.concatenate(tcls_list, 0).astype(int) if tcls_list else np.zeros((0,), dtype=int)\n",
    "                    _, _, P, R, mAP50, mAP5095 = compute_ap(tp, conf, pcls, tcls)\n",
    "                else:\n",
    "                    P = R = mAP50 = mAP5095 = float(\"nan\")\n",
    "                    tcls = np.zeros((0,), dtype=int)\n",
    "        \n",
    "                val_images = len(val_loader.dataset)\n",
    "                val_instances = int(tcls.shape[0])\n",
    "                print(f\"\\n{'':>19}{'all':>7}{val_images:>11}{val_instances:>12}\"\n",
    "                      f\"{P:>11.3f}{R:>11.3f}{mAP50:>11.3f}{mAP5095:>11.3f}\\n\")\n",
    "        \n",
    "            # ------------------- Save & early stop -------------------\n",
    "            torch.save(self.state_dict(), last_path)\n",
    "            improved = val_loss < best_val - 1e-6\n",
    "            if improved:\n",
    "                best_val, bad_epochs = val_loss, 0\n",
    "                torch.save(self.state_dict(), best_path)\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "        \n",
    "            if bad_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        return self\n",
    "        # return {\"best_val_loss\": best_val, \"best\": best_path, \"last\": last_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:02:38.796342Z",
     "iopub.status.busy": "2025-09-16T14:02:38.795340Z",
     "iopub.status.idle": "2025-09-16T14:02:38.814791Z",
     "shell.execute_reply": "2025-09-16T14:02:38.814175Z",
     "shell.execute_reply.started": "2025-09-16T14:02:38.796296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# yaml_text = \"\"\"\\\n",
    "# min_lr: 0.000100000000            # initial learning rate\n",
    "# max_lr: 0.010000000000            # maximum learning rate\n",
    "# momentum: 0.9370000000            # SGD momentum/Adam beta1\n",
    "# weight_decay: 0.000500            # optimizer weight decay\n",
    "# warmup_epochs: 3.00000            # warmup epochs\n",
    "# box: 7.500000000000000            # box loss gain\n",
    "# cls: 0.500000000000000            # cls loss gain\n",
    "# dfl: 1.500000000000000            # dfl loss gain\n",
    "# hsv_h: 0.0150000000000            # image HSV-Hue augmentation (fraction)\n",
    "# hsv_s: 0.7000000000000            # image HSV-Saturation augmentation (fraction)\n",
    "# hsv_v: 0.4000000000000            # image HSV-Value augmentation (fraction)\n",
    "# degrees: 0.00000000000            # image rotation (+/- deg)\n",
    "# translate: 0.100000000            # image translation (+/- fraction)\n",
    "# scale: 0.5000000000000            # image scale (+/- gain)\n",
    "# shear: 0.0000000000000            # image shear (+/- deg)\n",
    "# flip_ud: 0.00000000000            # image flip up-down (probability)\n",
    "# flip_lr: 0.50000000000            # image flip left-right (probability)\n",
    "# mosaic: 1.000000000000            # image mosaic (probability)\n",
    "# mix_up: 0.000000000000            # image mix-up (probability)\n",
    "# names:\n",
    "#   0: bacterial\n",
    "#   1: fungal\n",
    "#   2: pest\n",
    "#   3: physio\n",
    "# \"\"\"\n",
    "\n",
    "# path = Path(\"/kaggle/working/args.yaml\")  # change if not on Kaggle\n",
    "# path.write_text(yaml_text)\n",
    "# print(\"Saved to:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:02:38.815654Z",
     "iopub.status.busy": "2025-09-16T14:02:38.815450Z",
     "iopub.status.idle": "2025-09-16T14:02:44.852428Z",
     "shell.execute_reply": "2025-09-16T14:02:44.851277Z",
     "shell.execute_reply.started": "2025-09-16T14:02:38.815638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/effyolo /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:02:44.854121Z",
     "iopub.status.busy": "2025-09-16T14:02:44.853773Z",
     "iopub.status.idle": "2025-09-16T14:02:44.858810Z",
     "shell.execute_reply": "2025-09-16T14:02:44.858219Z",
     "shell.execute_reply.started": "2025-09-16T14:02:44.854095Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# ROOT = Path(\"/kaggle/working/effyolo\")  # or local path to apple_leaf\n",
    "# IMG_TRAIN = ROOT / \"train/images\"\n",
    "# IMG_VALID = ROOT / \"valid/images\"\n",
    "# IMG_TEST  = ROOT / \"test/images\"\n",
    "\n",
    "# def collect_images(folder):\n",
    "#     return sorted(str(p) for p in folder.rglob(\"*\") if p.is_file())\n",
    "\n",
    "# files_train = collect_images(IMG_TRAIN)\n",
    "# files_valid = collect_images(IMG_VALID)\n",
    "# files_test  = collect_images(IMG_TEST)\n",
    "\n",
    "# print(len(files_train), \"train |\", len(files_valid), \"valid |\", len(files_test), \"test\")\n",
    "\n",
    "\n",
    "# # input_size for the model\n",
    "# input_size=640\n",
    "\n",
    "# # get params from yaml file\n",
    "# with open('/kaggle/working/args.yaml', errors='ignore') as f:\n",
    "#         params = yaml.safe_load(f)\n",
    "\n",
    "# train_data=Dataset(files_train,input_size,params,augment=False)\n",
    "# valid_data=Dataset(files_valid,input_size,params,augment=False)\n",
    "# test_data=Dataset(files_test,input_size,params,augment=False)\n",
    "\n",
    "# train_loader = DataLoader(train_data, batch_size=8, shuffle=True, num_workers=2, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "# valid_loader = DataLoader(valid_data, batch_size=8, shuffle=False, num_workers=2, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "# test_loader  = DataLoader(test_data, batch_size=8, shuffle=False, num_workers=2, pin_memory=True, collate_fn=Dataset.collate_fn)\n",
    "\n",
    "# print(f\"Train_loader : {len(train_loader)} batches\")\n",
    "# print(f\"Train_loader : {len(valid_loader)} batches\")\n",
    "# print(f\"Train_loader : {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:06:29.729815Z",
     "iopub.status.busy": "2025-09-16T14:06:29.729514Z",
     "iopub.status.idle": "2025-09-16T14:06:29.847599Z",
     "shell.execute_reply": "2025-09-16T14:06:29.846770Z",
     "shell.execute_reply.started": "2025-09-16T14:06:29.729794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 0\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from ultralytics import YOLO as UModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "u = UModel('yolov8n.pt')          # COCO-pretrained\n",
    "model = YOLO().to(device)\n",
    "model.model[22].stride = torch.tensor([8., 16., 32.], device=device)\n",
    "\n",
    "tgt_sd = model.state_dict()\n",
    "sd_src = u.model.state_dict()     # plain PyTorch state_dict\n",
    "sd_shape_match = OrderedDict((k, v) for k, v in sd_src.items()\n",
    "                             if k in tgt_sd and tgt_sd[k].shape == v.shape)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(sd_shape_match, strict=False)\n",
    "print(len(missing), len(unexpected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:06:30.512016Z",
     "iopub.status.busy": "2025-09-16T14:06:30.511616Z",
     "iopub.status.idle": "2025-09-16T14:06:30.555056Z",
     "shell.execute_reply": "2025-09-16T14:06:30.553937Z",
     "shell.execute_reply.started": "2025-09-16T14:06:30.511986Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2930697325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results = model.train(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/kaggle/working/effyolo/data.yaml'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m640\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/2310495101.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, mode, **ultra_kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# 2) Ultralytics-style API when kwargs are provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0multra_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ultra_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0multra_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# 3) Default: behave like model.train(True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/2310495101.py\u001b[0m in \u001b[0;36m_ultra_train\u001b[0;34m(self, data, epochs, imgsz, batch, name, project, device, patience, cos_lr, lr, weight_decay, workers, amp, grad_clip)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# build loaders (plug in YOUR dataset + collate_fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYoloDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# <-- your class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0mval_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mval_ds\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mYoloDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval_path\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/22149892.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, foldername, imgsz, names, augment)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfoldername\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/22149892.py\u001b[0m in \u001b[0;36mload_label\u001b[0;34m(filenames)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{os.path.dirname(filenames[0])}.cache'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "results = model.train(\n",
    "    data='/kaggle/working/effyolo/data.yaml',\n",
    "    epochs=200,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    name='fruit-disease-detector',\n",
    "    project='/kaggle/working/runs/train',\n",
    "    device='cuda',\n",
    "    patience=50,\n",
    "    cos_lr=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T04:43:29.664534Z",
     "iopub.status.busy": "2025-09-16T04:43:29.663877Z",
     "iopub.status.idle": "2025-09-16T04:44:28.755987Z",
     "shell.execute_reply": "2025-09-16T04:44:28.754990Z",
     "shell.execute_reply.started": "2025-09-16T04:43:29.664509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142/3959368685.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
      "/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 | loss: 1093.9884 | box: 2.2886 | cls: 1088.9510 | dfl: 2.7488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_142/3959368685.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m          \u001b[0;31m# <-- iterate the loader, not a single batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# targets is a dict of tensors from your collate_fn; ComputeLoss moves them to device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# model, loss and optimizer\n",
    "criterion=ComputeLoss(model.model, params)\n",
    "optimizer=torch.optim.AdamW(model.model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "\n",
    "num_epochs=150\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=(device.type == 'cuda'))\n",
    "autocast = torch.autocast('cuda', dtype=torch.float16, enabled=(device.type == 'cuda'))\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for imgs, targets in train_loader:          # <-- iterate the loader, not a single batch\n",
    "        imgs = imgs.to(device, non_blocking=True).float() / 255.0\n",
    "        # targets is a dict of tensors from your collate_fn; ComputeLoss moves them to device\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
    "            outputs = model(imgs)               # training mode → Head returns 3 feature maps\n",
    "            box_loss, cls_loss, dfl_loss = criterion(outputs, targets)\n",
    "            loss = box_loss + cls_loss + dfl_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # clip_gradients(model, max_norm=10.0)  # optional\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # if using your scheduler that steps per-iteration:\n",
    "        # scheduler.step(global_step, optimizer)\n",
    "        global_step += 1\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "          f\"| loss: {loss.item():.4f} \"\n",
    "          f\"| box: {box_loss.item():.4f} \"\n",
    "          f\"| cls: {cls_loss.item():.4f} \"\n",
    "          f\"| dfl: {dfl_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"/kaggle/working/yolo_weights.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8254015,
     "sourceId": 13035527,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
